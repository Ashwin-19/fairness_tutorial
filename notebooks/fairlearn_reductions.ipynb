{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dealing with Bias and Fairness in Data Science Systems\n",
    "## KDD 2020 Hands-on Tutorial\n",
    "### Pedro Saleiro, Kit Rodolfa, Rayid Ghani"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=red>In-Processing Fairness Improvement</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Install dependencies, import packages and data\n",
    "This is needed every time you open this notebook in colab to install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install aequitas\n",
    "!pip install fairlearn\n",
    "import yaml\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from aequitas.group import Group\n",
    "from aequitas.bias import Bias\n",
    "from aequitas.fairness import Fairness\n",
    "import aequitas.plot as ap\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import fairlearn\n",
    "DATAPATH = 'https://github.com/dssg/fairness_tutorial/raw/master/data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's use the methods \n",
    "from fairlearn.reductions import ExponentiatedGradient, GridSearch, DemographicParity, TruePositiveRateDifference\n",
    "from fairlearn.metrics import selection_rate_group_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What has already happened?\n",
    "\n",
    "We've already cleaned data, generated features, created train-test sets, built 1000s of models on each training set and scored each test set with them, and calculated various evaluation metrics. We then used these results to pick a \"best\" model in terms of performance on the \"accuracy\" metric we care about: **Precision at the top 1000** (corresponding to our goal of selecting 1000 project submissions that are most likely to not get funded in order to prioritize resource allocation).\n",
    "\n",
    "When we audited this selected model with Aequitas, however, we found biases across many attributes, including the poverty level of the schools. Here, we explore a method of using in-processing to train a fairness-aware classifier in order to reduce this bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=green>FairLearn - a reductions approach</font>\n",
    "\n",
    "[Paper](https://arxiv.org/pdf/1803.02453.pdf): _A Reductions Approach to Fair Classification_, 2018\n",
    "\n",
    "> We present a systematic approach for achievingfairness in a binary classification setting. Whilewe focus on two well-known quantitative defini-tions of fairness, our approach encompasses manyother  previously  studied  definitions  as  specialcases. The key idea is to __reduce fair classification__ to a __sequence  of  cost-sensitive__  classification problems, whose solutions yield a randomized classifier with the __lowest (empirical) error__ subject to  the  __desired  constraints__.   We  introduce  two reductions that work for any representation of the cost-sensitive  classifier  and  compare  favorably to prior baselines on a variety of data sets, while overcoming several of their disadvantages.\n",
    "\n",
    "[FairLearn Documentation](https://fairlearn.github.io/user_guide/mitigation.html#id17)\n",
    "\n",
    "\n",
    "\n",
    "## TLDR; \n",
    "\n",
    "- This approach poses Fair Learning as a constrained optimization problem: minimize the empirical error, subject to linear constraints of the fairness (e.g., TPR difference, demographic parity).\n",
    "- Solve the constrained optimization as a __cost-sensitive__ classification problem.\n",
    "- Obtain a __randomized classifier__, which implies they will create multiple base estimators.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load train and test matrices as well as protected attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindf = pd.read_csv(DATAPATH + 'train_20120501_20120801.csv.gz', compression='gzip')\n",
    "testdf = pd.read_csv(DATAPATH + 'test_20121201_20130201.csv.gz', compression='gzip')\n",
    "train_attrdf = pd.read_csv(DATAPATH + 'train_20120501_20120801_protected.csv.gz', compression='gzip')\n",
    "test_attrdf = pd.read_csv(DATAPATH + 'test_20121201_20130201_protected.csv.gz', compression='gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up some parameters we'll need below and create matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_col = 'quickstart_label'\n",
    "date_col = 'as_of_date'\n",
    "id_col = 'entity_id'\n",
    "attr_col = 'poverty_level'\n",
    "exclude_cols = [label_col, date_col, id_col]\n",
    "\n",
    "top_k = 1000\n",
    "\n",
    "# aequitas parameters\n",
    "metrics = ['tpr']\n",
    "disparity_threshold = 1.3\n",
    "protected_attribute_ref_group = {attr_col:'lower'}\n",
    "\n",
    "\n",
    "X_train, y_train, A_train = traindf[[c for c in traindf.columns if c not in exclude_cols]].values, traindf[label_col].values, train_attrdf[[attr_col]]\n",
    "X_test,   y_test,   A_test   = testdf[[c for c in testdf.columns if c not in exclude_cols]].values,   testdf[label_col].values  , test_attrdf[[attr_col]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up a fairness-improving classifier\n",
    "\n",
    "To account fairness during model training, we'll use the **Exponentiated Gradient** provided by the `fairlearn` module.\n",
    "\n",
    "\n",
    "Its hyperparameters are: \n",
    "- `estimator`: an estimator that implements the methods `fit(X, y, sample_weight)` and `predict(X)`.\n",
    "- `constraints`: fairness constraints.\n",
    "- `eps: float`: fairness threshold, i.e., how much constraint violation we support (defaults to 0.01). \n",
    "- `T: int`: maximum number of iterations (defaults to 50).\n",
    "- `nu: float`: convergence threshold for duality gap (defaults to None).\n",
    "- `eta_0: float`: initial learning rate (defaults to 2).\n",
    "- `run_linprog_step: bool`: whether to apply saddle point optimization to the convex hull of classifiers obtained so far, after each exponentiated gradient step (defaults to True)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: Exponentiated Gradient has a stoachastic component\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we're using `TruePositiveRateDifference` for our fairness constraint here since we care about equalizing **recall** (aka **tpr** aka **equality of opportunity**) across our subgroups:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Step 1. Define the constraint\n",
    "constraint = TruePositiveRateDifference()\n",
    "\n",
    "# Step 2. Define the base estimator (any estimator providing 'fit' and 'predict')\n",
    "# Note: we could have used other algorithm such as logistic regression or random forest\n",
    "base_estimator = DecisionTreeClassifier(max_depth=20, min_samples_leaf=10)\n",
    "\n",
    "# Step 3. Define the bias reducer algorithm you want to apply\n",
    "bias_reducer = ExponentiatedGradient(base_estimator, constraint, T=50)\n",
    "\n",
    "# Step 4. Fit the data (and provide the sensitive attributes)\n",
    "bias_reducer.fit(X_train, y_train, sensitive_features=A_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict on our test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5. Use the mitigator to make predictions \n",
    "y_pred = bias_reducer.predict(X_test)\n",
    "new_preds = testdf[['entity_id','as_of_date','quickstart_label']].copy()\n",
    "new_preds['score'] = y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look at the output\n",
    "Notice that unlike many classifiers, the `ExponentiatedGradient` doesn't have a method for predicting a continuous score, just predicted classes of 0 or 1. How many projects did this model predict are at risk of going unfunded (that is, predicted class of 1)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_preds['score'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like about 6,500 projects are predicted as being at risk of going unfunded by this classifier, but unfortunately our program is resource-constrained and can only help 1,000 of them. At this point, if we wanted to pick out the 1,000 highest-risk projects (subject to our fairness constraint), we're a bit stuck: **the classifier doesn't give us any method for distinguishing higher-risk vs lower-risk projects!**\n",
    "\n",
    "Given this limitation, we might posit that a reasonable approach would be to pick 1,000 projects to intervene with from among these 6,500. Let's see what would happen if we did that..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Aequitas\n",
    "\n",
    "For reference, let's start with looking at the \"best\" model we chose in terms of overall precision at 1,000:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the predictions from the \"best\" model chosen earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_preds = pd.read_csv(DATAPATH + 'predictions_c598fbe93f4c218ac7d325fb478598f1.csv.gz', compression='gzip')\n",
    "old_attrdf = pd.read_csv(DATAPATH + 'test_20121201_20130201_protected.csv.gz', compression='gzip')\n",
    "\n",
    "old_df = pd.merge(old_preds, old_attrdf, how='left', on=[id_col, date_col], left_index=True, right_index=False, sort=True, copy=True)\n",
    "\n",
    "old_df = old_df.sort_values('predict_proba', ascending=False)\n",
    "old_df = old_df.rename(columns = {label_col:'label_value'}) # naming for Aequitas\n",
    "\n",
    "# create a \"score\" column with the predicted class (named \"score\" for use with Aequitas below)\n",
    "old_df['score'] = old_df.apply(lambda x: 1.0 if x.name in old_df.head(top_k).index.tolist() else 0, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Aequitas for the \"best\" model chosen earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = Group()\n",
    "b = Bias()\n",
    "\n",
    "xtab_old, _ = g.get_crosstabs(old_df[['score', 'label_value', attr_col]].copy())\n",
    "bdf_old = b.get_disparity_predefined_groups(xtab_old, original_df=old_df, ref_groups_dict=protected_attribute_ref_group)\n",
    "\n",
    "ap.disparities_metrics(bdf_old, metrics, attr_col, fairness_threshold=disparity_threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Aequitas for the new, fairness-aware model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember here that we would choose 1,000 at random from the 6,500 with predicted class 1, so the expected value for the recall disparity of this randomly-selected set would just be the value of full set (that is, the recall of each subgroup would, on average, be proportionally lower in the sub-sample):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(new_preds, test_attrdf, how='left', on=['entity_id','as_of_date'], left_index=True, right_index=False, sort=True, copy=True)\n",
    "df = df.rename(columns = {label_col:'label_value'})\n",
    "\n",
    "g = Group()\n",
    "b = Bias()\n",
    "\n",
    "xtab, _ = g.get_crosstabs(df[['score','label_value',attr_col]].copy())\n",
    "bdf = b.get_disparity_predefined_groups(xtab, original_df=df, ref_groups_dict=protected_attribute_ref_group)\n",
    "\n",
    "ap.disparities(bdf, metrics, attr_col, fairness_threshold=disparity_threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That looks pretty good! The new model appears to have reduced the disparity across poverty levels considerably relative to what we saw when training a model without a fairness constraint and choosing based on precision alone.\n",
    "\n",
    "However, the natural question here is where there is a fairness-accuracy trade-off here: What cost did we incur in terms of model performance, that is overall precision?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision of the \"best\" model chosen earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_df.loc[old_df['score']==1]['label_value'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision of the new, fairness-aware model\n",
    "\n",
    "As above, we would be sampling from the 6,500 down to 1,000 but the expected value of precision in this sample would just be the mean label value in the full population:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['score']==1]['label_value'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, compared to the old model, this method has resulted in **quite a large trade-off in model performance to acheive fairness**. This is certainly in part a result of the lack of flexibility in the method not allowing us to provide a score threshold or top k size that we're interested in equalizing our fairness metric around and instead using a built-in threshold that yields 6,500 predicted positives when we're only able to intervene on 1,000. Unfortunately, this sort of inflexibility appears to be a common attribute of many in-processing methods available today.\n",
    "\n",
    "For context, the overall base rate (`df['label_value'].mean()`) is 0.338, so the drop-off in precision here is about half way from our previous model to simply choosing at random."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding to model selection\n",
    "\n",
    "Finally, let's look at how this new option stacks up against what we plotted in our model selection process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
