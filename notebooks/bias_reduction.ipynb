{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dealing with Bias and Fairness in Data Science Systems\n",
    "## KDD 2020 Hands-on Tutorial\n",
    "### Pedro Saleiro, Kit Rodolfa, Rayid Ghani"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=red>Exploring Bias Reduction Strategies</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install dependencies, import packages and data\n",
    "This is needed every time you open this notebook in **colab** to install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install aequitas\n",
    "!pip install fairlearn\n",
    "import yaml\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from aequitas.group import Group\n",
    "from aequitas.bias import Bias\n",
    "from aequitas.fairness import Fairness\n",
    "import aequitas.plot as ap\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import fairlearn\n",
    "sns.set() \n",
    "DPI = 200\n",
    "DATAPATH = 'https://github.com/dssg/fairness_tutorial/raw/master/data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What has already happened?\n",
    "\n",
    "We've already cleaned data, generated features, created train-test sets, built 1000s of models on each training set and scored each test set with them, and calculated various evaluation metrics. \n",
    "\n",
    "As described earlier, the goal here is to select top 1000 project submissions that are likely to not get funded in order to prioritize resource allocation. That corresponds to the metric **Preicision at top 1000**.\n",
    "\n",
    "### <font color=red>We audited the \"best\" model at precision at top 1000 and found that it has disparities for True Positive Rate for all attributes that we care about (poverty_level of the school, sex, and school_location_type)</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What do we want to do now?\n",
    "\n",
    "1. Could we have picked a different model that was similar enough in \"precision at top 1000\" but less biased?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load predictions, labels, and attributes for all models that were built to audit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evals_df = pd.read_csv(DATAPATH +'split2_evals.csv.gz', compression='gzip')\n",
    "evals_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spot-check the model with highest precision at 1000 to see what type it is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evals_df['hyperparameters'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load pre-computed Aequitas audit results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aequitas_df = pd.read_csv(DATAPATH + 'split2_aequitas.csv.gz', compression='gzip')\n",
    "aequitas_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method will help us  plot the 400 models we trained and tested to see what are the current bias-performance tradeoffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_scatter_disparity_performance(evals_df, aequitas_df,  attr_col, group_name, \n",
    "                                         performance_col='model_precision', bias_metric='tpr', flip_disparity=False, \n",
    "                                         mitigated_tag=None, mitigated_bdf=pd.DataFrame(), mitigated_performance=None, ylim=None):\n",
    "    disparity_df = aequitas_df.loc[(aequitas_df['attribute_name']==attr_col) & (aequitas_df['attribute_value']==group_name)].copy()\n",
    "    disparity_metric = bias_metric + '_disparity'\n",
    "    scatter_schema = ['model_uuid', performance_col, 'attribute_name', 'attribute_value', bias_metric, disparity_metric, 'model_tag']\n",
    "    if flip_disparity:\n",
    "        disparity_df[disparity_metric]= disparity_df.apply(lambda x: 1/x[disparity_metric] , axis=1)\n",
    "    scatter = pd.merge(evals_df, disparity_df, how='left', on=['model_uuid'], left_index=True, sort=True, copy=True)\n",
    "    scatter = scatter[['model_uuid', performance_col, 'attribute_name', 'attribute_value', bias_metric, disparity_metric]].copy()\n",
    "    scatter['model_tag'] = 'Other Models'\n",
    "    scatter.sort_values('model_precision', ascending = False, inplace=True, ignore_index=True)\n",
    "    scatter['model_tag'] = scatter.apply(lambda x: 'Highest Precision at 1000' if int(x.name) < 1 else x['model_tag'], axis=1)\n",
    "    if not mitigated_bdf.empty and mitigated_performance !=None:\n",
    "      mitigated_bdf[performance_col] = mitigated_performance\n",
    "      mitigated_bdf['model_tag'] = mitigated_tag\n",
    "      new_disparity_df = mitigated_bdf.loc[(mitigated_bdf['attribute_name']==attr_col) & (mitigated_bdf['attribute_value']==group_name)].copy()\n",
    "      scatter_new = new_disparity_df[[c for c in new_disparity_df.columns if c in scatter_schema]]\n",
    "      scatter_final = pd.concat([scatter, scatter_new], axis=0)\n",
    "    else:\n",
    "      scatter_final = scatter.copy()\n",
    "    ax = sns.scatterplot(\n",
    "        x='model_precision', y=disparity_metric, hue='model_tag',\n",
    "        data=scatter_final,\n",
    "        alpha=0.5, s=20,\n",
    "    )\n",
    "    if ylim:\n",
    "        plt.ylim(0, 10)\n",
    "    flip_placeholder = 'Flipped' if flip_disparity else ''\n",
    "    ax.set_title('{} {} vs.{} for {}:{}'.format(flip_placeholder, disparity_metric, performance_col, attr_col,group_name ), y=1.)\n",
    "    plt.gcf().set_size_inches((4, 3))\n",
    "    plt.legend(loc='upper left', fontsize='xx-small')\n",
    "\n",
    "    plt.gcf().set_dpi(DPI)\n",
    "    plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "## Let's see if we could have picked a better model for fairness in Poverty Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_scatter_disparity_performance(evals_df, aequitas_df, 'poverty_level', 'highest', flip_disparity=True )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's see if we could have picked a better model for fairness in Metro Type\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_scatter_disparity_performance(evals_df, aequitas_df, 'metro_type', 'urban', flip_disparity=True, ylim=None )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's see if we could have picked a better model for fairness in Teacher sex\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_scatter_disparity_performance(evals_df, aequitas_df, 'teacher_sex', 'female', flip_disparity=False )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bias Reduction Strategies\n",
    "We will try a few different bias reduction strategies now and compare the audit results with the original models\n",
    "\n",
    "1. Re-Sampling\n",
    "2. Regularization (using Fairlearn package)\n",
    "3. Post-hoc adjustment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"red\">Bias Reduction Strategy 1: Re-Sampling</font>\n",
    "### Can resampling approaches help improve the fairness of our models?\n",
    "\n",
    "1. Load data\n",
    "2. Look at training data distributions\n",
    "3. Try Resampling in a few different ways\n",
    "4. Rebuild model(s) on resampled training data\n",
    "5. Predict on the test set\n",
    "6. Audit for Bias and Compare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What has already happened?\n",
    "\n",
    "We've already cleaned data, generated features, created train-test sets, built 1000s of models on each training set and scored each test set with them, and calculated various evaluation metrics. We then used these results to pick a \"best\" model in terms of performance on the \"accuracy\" metric we care about: **Precision at the top 1000** (corresponding to our goal of selecting 1000 project submissions that are most likely to not get funded in order to prioritize resource allocation).\n",
    "\n",
    "When we audited this selected model with Aequitas, however, we found biases across many attributes, including the poverty level of the schools. Here, we explore a methods based on resampling to reduce this bias in the selected model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindf = pd.read_csv(DATAPATH + 'train_20120501_20120801.csv.gz', compression='gzip')\n",
    "testdf = pd.read_csv(DATAPATH + 'test_20121201_20130201.csv.gz', compression='gzip')\n",
    "train_attrdf = pd.read_csv(DATAPATH + 'train_20120501_20120801_protected.csv.gz', compression='gzip')\n",
    "test_attrdf = pd.read_csv(DATAPATH + 'test_20121201_20130201_protected.csv.gz', compression='gzip')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16790, 113)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traindf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "highest    9448\n",
       "lower      7342\n",
       "Name: poverty_level, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_attrdf['poverty_level'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load pre-built models and predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "evals_df = pd.read_csv(DATAPATH +'split2_evals.csv.gz', compression='gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take a look at the \"Best\" performing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_precision</th>\n",
       "      <th>model_classpath</th>\n",
       "      <th>hyperparameters</th>\n",
       "      <th>model_uuid</th>\n",
       "      <th>predictions_uuid</th>\n",
       "      <th>target_pp</th>\n",
       "      <th>matrix_type</th>\n",
       "      <th>matrix_start_date</th>\n",
       "      <th>matrix_end_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.552</td>\n",
       "      <td>sklearn.ensemble.RandomForestClassifier</td>\n",
       "      <td>{\"n_jobs\": -1, \"criterion\": \"gini\", \"max_depth...</td>\n",
       "      <td>a04e2eedd9c5ff18bcf77e84ae9db561</td>\n",
       "      <td>c598fbe93f4c218ac7d325fb478598f1</td>\n",
       "      <td>1000</td>\n",
       "      <td>test</td>\n",
       "      <td>2012-12-01</td>\n",
       "      <td>2013-01-31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   model_precision                          model_classpath  \\\n",
       "0            0.552  sklearn.ensemble.RandomForestClassifier   \n",
       "\n",
       "                                     hyperparameters  \\\n",
       "0  {\"n_jobs\": -1, \"criterion\": \"gini\", \"max_depth...   \n",
       "\n",
       "                         model_uuid                  predictions_uuid  \\\n",
       "0  a04e2eedd9c5ff18bcf77e84ae9db561  c598fbe93f4c218ac7d325fb478598f1   \n",
       "\n",
       "   target_pp matrix_type matrix_start_date matrix_end_date  \n",
       "0       1000        test        2012-12-01      2013-01-31  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evals_df[evals_df['model_uuid']=='a04e2eedd9c5ff18bcf77e84ae9db561']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "hyperparameters= ast.literal_eval(evals_df['hyperparameters'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(**hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'base_estimator': DecisionTreeClassifier(),\n",
       " 'n_estimators': 87,\n",
       " 'estimator_params': ('criterion',\n",
       "  'max_depth',\n",
       "  'min_samples_split',\n",
       "  'min_samples_leaf',\n",
       "  'min_weight_fraction_leaf',\n",
       "  'max_features',\n",
       "  'max_leaf_nodes',\n",
       "  'min_impurity_decrease',\n",
       "  'min_impurity_split',\n",
       "  'random_state',\n",
       "  'ccp_alpha'),\n",
       " 'bootstrap': True,\n",
       " 'oob_score': False,\n",
       " 'n_jobs': -1,\n",
       " 'random_state': 213500298,\n",
       " 'verbose': 0,\n",
       " 'warm_start': False,\n",
       " 'class_weight': None,\n",
       " 'max_samples': None,\n",
       " 'criterion': 'gini',\n",
       " 'max_depth': 30,\n",
       " 'min_samples_split': 3,\n",
       " 'min_samples_leaf': 44,\n",
       " 'min_weight_fraction_leaf': 0.0,\n",
       " 'max_features': 'sqrt',\n",
       " 'max_leaf_nodes': None,\n",
       " 'min_impurity_decrease': 0.0,\n",
       " 'min_impurity_split': None,\n",
       " 'ccp_alpha': 0.0}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf.__dict__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at training data distributions\n",
    "\n",
    "#### Poverty_level=Highest\n",
    "label_pos_poverty_highest =  P(poverty_level=highest | not_funded)\n",
    "\n",
    "label_neg_poverty_highest =  P(poverty_level=highest | funded)\n",
    "\n",
    "\n",
    "#### Poverty_level=Lower\n",
    "label_pos_poverty_lower =  P(poverty_level=lower | not_funded)\n",
    "\n",
    "label_neg_poverty_lower =  P(poverty_level=lower | funded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_pos_poverty_highest = traindf.loc[(train_attrdf['poverty_level']=='highest') & (traindf['quickstart_label'] > 0)]\n",
    "label_neg_poverty_highest = traindf.loc[(train_attrdf['poverty_level']=='highest') & (traindf['quickstart_label'] < 1.0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_pos_poverty_lower = traindf.loc[(train_attrdf['poverty_level']=='lower') & (traindf['quickstart_label'] > 0)]\n",
    "label_neg_poverty_lower = traindf.loc[(train_attrdf['poverty_level']=='lower') & (traindf['quickstart_label'] <1.0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3196, 113)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_pos_poverty_highest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6252, 113)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_neg_poverty_highest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3130, 113)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_pos_poverty_lower.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4212, 113)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_neg_poverty_lower.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Highest default training prevalence: 0.338272650296359\n"
     ]
    }
   ],
   "source": [
    "print('Highest Poverty training set prevalence:', len(label_pos_poverty_highest) / len(train_attrdf[train_attrdf['poverty_level']=='highest']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lower default training prevalence: 0.42631435576137294\n"
     ]
    }
   ],
   "source": [
    "print('Lower Poverty training set prevalence:', len(label_pos_poverty_lower) / len(train_attrdf[train_attrdf['poverty_level']=='lower']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What type of disparities do we see in the data distribution here?\n",
    "\n",
    "\n",
    "1.\n",
    "\n",
    "2.\n",
    "\n",
    "3.\n",
    "\n",
    "4.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "...\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's now try resampling\n",
    "\n",
    "We can perform three types of resampling:\n",
    "\n",
    "1. Change the training data such that different poverty levels are distributed more uniformly but keep the distribution of labels the same within each poverty level P(poverty_level = highest) = P (poverty_level=lower)\n",
    "\n",
    "\n",
    "2. Change the training data such that different poverty levels have more uniform label distributions P(poverty_level = highest | not funded ) = P(poverty_level=lower | not funded)\n",
    "\n",
    "\n",
    "3. Change both\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach 2: Change the training data such that different poverty levels have more uniform label distributions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Highest new training prevalence: 0.375\n"
     ]
    }
   ],
   "source": [
    "n_pos_highest = 3000\n",
    "n_neg_highest = 5000\n",
    "print('Highest new training prevalence:', n_pos_highest / (n_pos_highest + n_neg_highest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lower new training prevalence: 0.38461538461538464\n"
     ]
    }
   ],
   "source": [
    "n_pos_lower = 2500\n",
    "n_neg_lower = 4000\n",
    "print('Lower new training prevalence:', n_pos_lower / (n_pos_lower + n_neg_lower))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_pos_poverty_highest = label_pos_poverty_highest.sample(n=n_pos_highest, replace=False)\n",
    "sample_neg_poverty_highest = label_neg_poverty_highest.sample(n=n_neg_highest, replace=False)\n",
    "\n",
    "sample_pos_poverty_lower = label_pos_poverty_lower.sample(n=n_pos_lower, replace=False)\n",
    "sample_neg_poverty_lower = label_neg_poverty_lower.sample(n=n_neg_lower, replace=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rebuild model on resampled training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(max_depth=30, max_features='sqrt', min_samples_leaf=44,\n",
       "                       min_samples_split=3, n_estimators=87, n_jobs=-1,\n",
       "                       random_state=213500298)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_traindf = pd.concat([sample_pos_poverty_highest,sample_neg_poverty_highest,sample_pos_poverty_lower, sample_neg_poverty_lower], axis=0)\n",
    "y_train = new_traindf['quickstart_label'].values\n",
    "rf.fit(new_traindf.drop(['entity_id','as_of_date','quickstart_label'], axis = 1), y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict on the test set and calculate precision at 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Precision:  0.567\n"
     ]
    }
   ],
   "source": [
    "y_pred = rf.predict_proba(testdf.drop(['entity_id','as_of_date','quickstart_label'], axis = 1))[:,1]\n",
    "new_preds = testdf[['entity_id','as_of_date','quickstart_label']].copy()\n",
    "new_preds['predict_proba'] = y_pred\n",
    "new_preds = new_preds.sort_values('predict_proba', ascending = False).reset_index(drop=True).copy()\n",
    "new_preds['score'] = new_preds.apply(lambda x: 1.0 if int(x.name)  < 1000 else 0.0, axis=1)\n",
    "print('Model Precision: ', new_preds[new_preds['score'] > 0]['quickstart_label'].sum() / 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Audit for Bias  (keeping the attributes, reference groups, bias metric, and tolerance the same as before)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_id, score_thresholds 0 {'rank_abs': [1000]}\n"
     ]
    }
   ],
   "source": [
    "df = pd.merge(new_preds, test_attrdf, how='left', on=['entity_id','as_of_date'], left_index=True, right_index=False, sort=True, copy=True)\n",
    "df = df.rename(columns = {'quickstart_label':'label_value'})\n",
    "metrics = ['tpr']\n",
    "g = Group()\n",
    "xtab, _ = g.get_crosstabs(df[['score','label_value','poverty_level','metro_type', 'teacher_sex']].copy())\n",
    "b = Bias()\n",
    "bdf = b.get_disparity_predefined_groups(xtab, original_df=df, \n",
    "                                        ref_groups_dict={'poverty_level':'lower', 'metro_type':'suburban_rural', 'teacher_sex':'male'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Before and After: Look at disparities and compare to the original version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_disparity_predefined_group()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<div id=\"altair-viz-07bec70db4124b68b901618269bb0b44\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-07bec70db4124b68b901618269bb0b44\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-07bec70db4124b68b901618269bb0b44\");\n",
       "    }\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.8.1?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function loadScript(lib) {\n",
       "      return new Promise(function(resolve, reject) {\n",
       "        var s = document.createElement('script');\n",
       "        s.src = paths[lib];\n",
       "        s.async = true;\n",
       "        s.onload = () => resolve(paths[lib]);\n",
       "        s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "        document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "      });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else if (typeof vegaEmbed === \"function\") {\n",
       "      displayChart(vegaEmbed);\n",
       "    } else {\n",
       "      loadScript(\"vega\")\n",
       "        .then(() => loadScript(\"vega-lite\"))\n",
       "        .then(() => loadScript(\"vega-embed\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300, \"strokeWidth\": 0}, \"axisLeft\": {\"labelColor\": \"rgb(117,117,117)\", \"labelFont\": \"Helvetica\", \"labelFontSize\": 16}}, \"layer\": [{\"data\": {\"name\": \"data-b3bcfd1decd67fb31f95ea5e1bcd9e64\"}, \"mark\": {\"type\": \"rule\", \"stroke\": \"rgb(117,117,117)\", \"strokeWidth\": 1.25, \"tooltip\": \"\"}, \"encoding\": {\"x\": {\"type\": \"quantitative\", \"field\": \"x\", \"scale\": {\"domain\": [-2, 2], \"range\": [80.0, 720.0]}}, \"x2\": {\"field\": \"x2\"}, \"y\": {\"type\": \"nominal\", \"axis\": {\"domain\": false, \"labelAngle\": 0, \"labelPadding\": -30, \"orient\": \"left\", \"ticks\": false, \"title\": \"\"}, \"field\": \"metric\", \"scale\": {\"domain\": [\"TPR\"], \"range\": [30, 225.0]}}}}, {\"data\": {\"name\": \"data-79dd248bdaa0ea02c535240e8ad022c8\"}, \"mark\": {\"type\": \"rule\", \"stroke\": \"rgb(117,117,117)\", \"strokeDash\": [5, 5], \"strokeWidth\": 1}, \"encoding\": {\"tooltip\": {\"value\": \"lower [REF]\"}, \"x\": {\"value\": 400.0}, \"y\": {\"value\": 20.833333333333336}, \"y2\": {\"value\": 229.16666666666666}}}, {\"data\": {\"name\": \"data-bf3bbf11504879c3d6556966f4853596\"}, \"mark\": {\"type\": \"text\", \"align\": \"center\", \"color\": \"rgb(117,117,117)\", \"font\": \"Helvetica\", \"fontSize\": 11, \"fontWeight\": 300, \"tooltip\": \"\"}, \"encoding\": {\"text\": {\"type\": \"nominal\", \"field\": \"label\"}, \"x\": {\"type\": \"quantitative\", \"field\": \"value\", \"scale\": {\"domain\": [-2, 2], \"range\": [80.0, 720.0]}}, \"y\": {\"value\": 17.5}}}, {\"layer\": [{\"mark\": {\"type\": \"text\", \"align\": \"right\", \"fill\": \"rgb(117,117,117)\", \"font\": \"Helvetica\", \"fontSize\": 11, \"fontWeight\": 300, \"tooltip\": \"\"}, \"encoding\": {\"text\": {\"value\": \"TIMES SMALLER\"}, \"x\": {\"value\": 120.0}, \"y\": {\"value\": 7.5}}}, {\"mark\": {\"type\": \"text\", \"align\": \"left\", \"fill\": \"rgb(117,117,117)\", \"font\": \"Helvetica\", \"fontSize\": 11, \"fontWeight\": 300, \"tooltip\": \"\"}, \"encoding\": {\"text\": {\"value\": \"TIMES LARGER\"}, \"x\": {\"value\": 680.0}, \"y\": {\"value\": 7.5}}}, {\"mark\": {\"type\": \"text\", \"align\": \"center\", \"fill\": \"rgb(117,117,117)\", \"font\": \"Helvetica\", \"fontSize\": 11, \"fontWeight\": 300, \"tooltip\": \"\"}, \"encoding\": {\"text\": {\"value\": \"EQUAL\"}, \"x\": {\"value\": 400.0}, \"y\": {\"value\": 7.5}}}], \"data\": {\"name\": \"data-79dd248bdaa0ea02c535240e8ad022c8\"}}, {\"layer\": [{\"mark\": {\"type\": \"rule\", \"opacity\": 0.8, \"stroke\": \"rgb(217, 46, 28)\", \"strokeWidth\": 1.25, \"tooltip\": \"\"}, \"encoding\": {\"x\": {\"type\": \"quantitative\", \"field\": \"min\", \"scale\": {\"domain\": [-2, 2], \"range\": [80.0, 720.0]}}, \"y\": {\"value\": 25.0}, \"y2\": {\"value\": 225.0}}}, {\"mark\": {\"type\": \"rule\", \"opacity\": 0.8, \"stroke\": \"rgb(217, 46, 28)\", \"strokeWidth\": 1.25, \"tooltip\": \"\"}, \"encoding\": {\"x\": {\"type\": \"quantitative\", \"field\": \"max\", \"scale\": {\"domain\": [-2, 2], \"range\": [80.0, 720.0]}}, \"y\": {\"value\": 25.0}, \"y2\": {\"value\": 225.0}}}, {\"layer\": [{\"mark\": {\"type\": \"rect\", \"fill\": \"rgb(217, 46, 28)\", \"opacity\": 0.1, \"tooltip\": \"\"}, \"encoding\": {\"x\": {\"type\": \"quantitative\", \"field\": \"min\", \"scale\": {\"domain\": [-2, 2], \"range\": [80.0, 720.0]}}, \"x2\": {\"field\": \"lower_end\"}, \"y\": {\"value\": 25.0}, \"y2\": {\"value\": 225.0}}}, {\"mark\": {\"type\": \"rect\", \"fill\": \"rgb(217, 46, 28)\", \"opacity\": 0.1, \"tooltip\": \"\"}, \"encoding\": {\"x\": {\"type\": \"quantitative\", \"field\": \"max\", \"scale\": {\"domain\": [-2, 2], \"range\": [80.0, 720.0]}}, \"x2\": {\"field\": \"upper_end\"}, \"y\": {\"value\": 25.0}, \"y2\": {\"value\": 225.0}}}]}, {\"data\": {\"name\": \"data-79dd248bdaa0ea02c535240e8ad022c8\"}, \"mark\": {\"type\": \"text\", \"align\": \"left\", \"baseline\": \"top\", \"fill\": \"rgb(217, 46, 28)\", \"font\": \"Helvetica\", \"fontSize\": 11, \"fontWeight\": 300, \"tooltip\": \"\"}, \"encoding\": {\"text\": {\"value\": \"The metric value for any group should not be 1.3 (or more) times smaller or larger than that of the reference group lower.\"}, \"x\": {\"value\": 0}, \"y\": {\"value\": 233.33333333333334}}}], \"data\": {\"name\": \"data-b643063563633c31a45d634da5f365b2\"}}, {\"layer\": [{\"mark\": \"circle\"}, {\"data\": {\"name\": \"data-6e482505e3df397bfd814f0eefc969eb\"}, \"mark\": {\"type\": \"circle\", \"opacity\": 0.2}, \"encoding\": {\"color\": {\"condition\": {\"type\": \"nominal\", \"field\": \"attribute_value\", \"legend\": null, \"scale\": {\"domain\": [\"lower\", \"highest\"], \"range\": [\"rgb(117,117,117)\", \"rgb(100, 143, 255)\", \"rgb(254, 97, 0)\", \"rgb(220, 38, 127)\", \"rgb(255, 176, 0)\", \"rgb(120, 94, 240)\"]}, \"selection\": \"selector017\"}, \"value\": \"rgb(224,224,224)\"}, \"size\": {\"type\": \"quantitative\", \"field\": \"group_size\", \"legend\": null, \"scale\": {\"domain\": [0, 11967.6], \"range\": [0, 7853.981633974483]}}, \"tooltip\": [{\"type\": \"nominal\", \"field\": \"attribute_value\", \"title\": \"Group\"}, {\"type\": \"nominal\", \"field\": \"tooltip_group_size\", \"title\": \"Group Size\"}, {\"type\": \"nominal\", \"field\": \"tooltip_disparity_explanation_tpr\", \"title\": \"Disparity\"}, {\"type\": \"quantitative\", \"field\": \"tpr\", \"format\": \".2f\", \"title\": \"TPR\"}], \"x\": {\"type\": \"quantitative\", \"axis\": {\"domain\": false, \"labels\": false, \"ticks\": false, \"title\": null, \"values\": [-2, -1, 1, 2]}, \"field\": \"tpr_disparity_scaled\", \"scale\": {\"domain\": [-2, 2], \"range\": [80.0, 720.0]}}, \"y\": {\"type\": \"nominal\", \"axis\": {\"domain\": false, \"grid\": false, \"labels\": false, \"ticks\": false, \"title\": \" \"}, \"field\": \"metric_variable\", \"scale\": {\"domain\": [\"TPR\"], \"range\": [30, 225.0]}}}, \"selection\": {\"selector019\": {\"type\": \"multi\", \"empty\": \"all\", \"fields\": [\"attribute_value\"]}}, \"transform\": [{\"calculate\": \"'TPR'\", \"as\": \"metric_variable\"}]}, {\"layer\": [{\"mark\": \"point\"}, {\"data\": {\"name\": \"data-6e482505e3df397bfd814f0eefc969eb\"}, \"mark\": {\"type\": \"point\", \"filled\": true, \"size\": 60}, \"encoding\": {\"color\": {\"condition\": {\"type\": \"nominal\", \"field\": \"attribute_value\", \"legend\": null, \"scale\": {\"domain\": [\"lower\", \"highest\"], \"range\": [\"rgb(117,117,117)\", \"rgb(100, 143, 255)\", \"rgb(254, 97, 0)\", \"rgb(220, 38, 127)\", \"rgb(255, 176, 0)\", \"rgb(120, 94, 240)\"]}, \"selection\": \"selector017\"}, \"value\": \"rgb(224,224,224)\"}, \"shape\": {\"type\": \"nominal\", \"field\": \"attribute_value\", \"legend\": null, \"scale\": {\"domain\": [\"lower\", \"highest\"], \"range\": [\"cross\", \"circle\"]}}, \"tooltip\": [{\"type\": \"nominal\", \"field\": \"attribute_value\", \"title\": \"Group\"}, {\"type\": \"nominal\", \"field\": \"tooltip_group_size\", \"title\": \"Group Size\"}, {\"type\": \"nominal\", \"field\": \"tooltip_disparity_explanation_tpr\", \"title\": \"Disparity\"}, {\"type\": \"quantitative\", \"field\": \"tpr\", \"format\": \".2f\", \"title\": \"TPR\"}], \"x\": {\"type\": \"quantitative\", \"axis\": {\"domain\": false, \"labels\": false, \"ticks\": false, \"title\": null, \"values\": [-2, -1, 1, 2]}, \"field\": \"tpr_disparity_scaled\", \"scale\": {\"domain\": [-2, 2], \"range\": [80.0, 720.0]}}, \"y\": {\"type\": \"nominal\", \"axis\": {\"domain\": false, \"grid\": false, \"labels\": false, \"ticks\": false, \"title\": \" \"}, \"field\": \"metric_variable\", \"scale\": {\"domain\": [\"TPR\"], \"range\": [30, 225.0]}}}, \"selection\": {\"selector018\": {\"type\": \"multi\", \"empty\": \"all\", \"fields\": [\"attribute_value\"]}}, \"transform\": [{\"calculate\": \"'TPR'\", \"as\": \"metric_variable\"}]}]}]}, {\"layer\": [{\"mark\": {\"type\": \"point\", \"filled\": true, \"tooltip\": \"\"}, \"encoding\": {\"color\": {\"condition\": {\"type\": \"nominal\", \"field\": \"attribute_value\", \"legend\": null, \"scale\": {\"domain\": [\"lower\", \"highest\"], \"range\": [\"rgb(117,117,117)\", \"rgb(100, 143, 255)\", \"rgb(254, 97, 0)\", \"rgb(220, 38, 127)\", \"rgb(255, 176, 0)\", \"rgb(120, 94, 240)\"]}, \"selection\": \"selector017\"}, \"value\": \"rgb(224,224,224)\"}, \"shape\": {\"type\": \"nominal\", \"field\": \"attribute_value\", \"legend\": null, \"scale\": {\"domain\": [\"lower\", \"highest\"], \"range\": [\"cross\", \"circle\"]}}, \"x\": {\"value\": 880.0000000000001}, \"y\": {\"type\": \"nominal\", \"axis\": {\"domain\": false, \"grid\": false, \"labels\": false, \"ticks\": false, \"title\": \" \"}, \"field\": \"attribute_value\", \"scale\": {\"domain\": [\"lower\", \"highest\"], \"range\": [21, 73]}}}, \"selection\": {\"selector017\": {\"type\": \"multi\", \"empty\": \"all\", \"fields\": [\"attribute_value\"]}}}, {\"mark\": {\"type\": \"text\", \"align\": \"left\", \"baseline\": \"middle\", \"font\": \"Helvetica\", \"fontSize\": 11, \"fontWeight\": 300, \"tooltip\": \"\"}, \"encoding\": {\"color\": {\"condition\": {\"type\": \"nominal\", \"field\": \"attribute_value\", \"legend\": null, \"scale\": {\"domain\": [\"lower\", \"highest\"], \"range\": [\"rgb(117,117,117)\", \"rgb(100, 143, 255)\", \"rgb(254, 97, 0)\", \"rgb(220, 38, 127)\", \"rgb(255, 176, 0)\", \"rgb(120, 94, 240)\"]}, \"selection\": \"selector017\"}, \"value\": \"rgb(224,224,224)\"}, \"text\": {\"type\": \"nominal\", \"field\": \"label\"}, \"x\": {\"value\": 890.0000000000001}, \"y\": {\"type\": \"nominal\", \"axis\": {\"domain\": false, \"grid\": false, \"labels\": false, \"ticks\": false, \"title\": \" \"}, \"field\": \"attribute_value\", \"scale\": {\"domain\": [\"lower\", \"highest\"], \"range\": [21, 73]}}}, \"selection\": {\"selector020\": {\"type\": \"multi\", \"empty\": \"all\", \"fields\": [\"attribute_value\"]}}}, {\"data\": {\"name\": \"data-79dd248bdaa0ea02c535240e8ad022c8\"}, \"mark\": {\"type\": \"text\", \"align\": \"left\", \"baseline\": \"middle\", \"color\": \"rgb(117,117,117)\", \"font\": \"Helvetica\", \"fontSize\": 11, \"fontWeight\": 300, \"tooltip\": \"\"}, \"encoding\": {\"text\": {\"value\": \"Click to highlight a group.\"}, \"x\": {\"value\": 880.0000000000001}, \"y\": {\"value\": 0}}}], \"data\": {\"name\": \"data-a5ebce136adcce28635ff1807ca7d132\"}}], \"height\": 250, \"resolve\": {\"scale\": {\"size\": \"independent\", \"y\": \"independent\"}}, \"width\": 800, \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.8.1.json\", \"datasets\": {\"data-b3bcfd1decd67fb31f95ea5e1bcd9e64\": [{\"metric\": \"TPR\", \"x\": -2.2, \"x2\": 2.2}], \"data-79dd248bdaa0ea02c535240e8ad022c8\": [{\"a\": 1, \"b\": 0}, {\"a\": 1, \"b\": 0}], \"data-bf3bbf11504879c3d6556966f4853596\": [{\"value\": -2, \"label\": 3}, {\"value\": -1, \"label\": 2}, {\"value\": 0, \"label\": \"=\"}, {\"value\": 1, \"label\": 2}, {\"value\": 2, \"label\": 3}], \"data-b643063563633c31a45d634da5f365b2\": [{\"min\": -0.30000000000000004, \"max\": 0.30000000000000004, \"lower_end\": -2.2, \"upper_end\": 2.2}], \"data-6e482505e3df397bfd814f0eefc969eb\": [{\"attribute_name\": \"poverty_level\", \"attribute_value\": \"highest\", \"group_size\": 9973, \"total_entities\": 17677, \"tpr\": 0.08483290488431877, \"tpr_disparity\": 0.8001730764336075, \"tpr_disparity_scaled\": -0.24972962656657538, \"tooltip_group_size\": \"9,973 (56.42%)\", \"tooltip_disparity_explanation_tpr\": \"1.25 times smaller TPR than the reference group\"}, {\"attribute_name\": \"poverty_level\", \"attribute_value\": \"lower\", \"group_size\": 7704, \"total_entities\": 17677, \"tpr\": 0.10601819454163751, \"tpr_disparity\": 1.0, \"tpr_disparity_scaled\": 0.0, \"tooltip_group_size\": \"7,704 (43.58%)\", \"tooltip_disparity_explanation_tpr\": \"Reference group\"}], \"data-a5ebce136adcce28635ff1807ca7d132\": [{\"attribute_value\": \"lower\", \"label\": \"lower [REF]\"}, {\"attribute_value\": \"highest\", \"label\": \"highest\"}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.LayerChart(...)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ap.disparities(bdf, metrics, 'poverty_level', fairness_threshold = 1.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div id=\"altair-viz-707f26990ce04ff7a1650f9f9393b8eb\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-707f26990ce04ff7a1650f9f9393b8eb\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-707f26990ce04ff7a1650f9f9393b8eb\");\n",
       "    }\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.8.1?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function loadScript(lib) {\n",
       "      return new Promise(function(resolve, reject) {\n",
       "        var s = document.createElement('script');\n",
       "        s.src = paths[lib];\n",
       "        s.async = true;\n",
       "        s.onload = () => resolve(paths[lib]);\n",
       "        s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "        document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "      });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else if (typeof vegaEmbed === \"function\") {\n",
       "      displayChart(vegaEmbed);\n",
       "    } else {\n",
       "      loadScript(\"vega\")\n",
       "        .then(() => loadScript(\"vega-lite\"))\n",
       "        .then(() => loadScript(\"vega-embed\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300, \"strokeWidth\": 0}, \"axisLeft\": {\"labelColor\": \"rgb(117,117,117)\", \"labelFont\": \"Helvetica\", \"labelFontSize\": 16}}, \"layer\": [{\"data\": {\"name\": \"data-b3bcfd1decd67fb31f95ea5e1bcd9e64\"}, \"mark\": {\"type\": \"rule\", \"stroke\": \"rgb(117,117,117)\", \"strokeWidth\": 1.25, \"tooltip\": \"\"}, \"encoding\": {\"x\": {\"type\": \"quantitative\", \"field\": \"x\", \"scale\": {\"domain\": [-2, 2], \"range\": [80.0, 720.0]}}, \"x2\": {\"field\": \"x2\"}, \"y\": {\"type\": \"nominal\", \"axis\": {\"domain\": false, \"labelAngle\": 0, \"labelPadding\": -30, \"orient\": \"left\", \"ticks\": false, \"title\": \"\"}, \"field\": \"metric\", \"scale\": {\"domain\": [\"TPR\"], \"range\": [30, 225.0]}}}}, {\"data\": {\"name\": \"data-79dd248bdaa0ea02c535240e8ad022c8\"}, \"mark\": {\"type\": \"rule\", \"stroke\": \"rgb(117,117,117)\", \"strokeDash\": [5, 5], \"strokeWidth\": 1}, \"encoding\": {\"tooltip\": {\"value\": \"suburban_rural [REF]\"}, \"x\": {\"value\": 400.0}, \"y\": {\"value\": 20.833333333333336}, \"y2\": {\"value\": 229.16666666666666}}}, {\"data\": {\"name\": \"data-bf3bbf11504879c3d6556966f4853596\"}, \"mark\": {\"type\": \"text\", \"align\": \"center\", \"color\": \"rgb(117,117,117)\", \"font\": \"Helvetica\", \"fontSize\": 11, \"fontWeight\": 300, \"tooltip\": \"\"}, \"encoding\": {\"text\": {\"type\": \"nominal\", \"field\": \"label\"}, \"x\": {\"type\": \"quantitative\", \"field\": \"value\", \"scale\": {\"domain\": [-2, 2], \"range\": [80.0, 720.0]}}, \"y\": {\"value\": 17.5}}}, {\"layer\": [{\"mark\": {\"type\": \"text\", \"align\": \"right\", \"fill\": \"rgb(117,117,117)\", \"font\": \"Helvetica\", \"fontSize\": 11, \"fontWeight\": 300, \"tooltip\": \"\"}, \"encoding\": {\"text\": {\"value\": \"TIMES SMALLER\"}, \"x\": {\"value\": 120.0}, \"y\": {\"value\": 7.5}}}, {\"mark\": {\"type\": \"text\", \"align\": \"left\", \"fill\": \"rgb(117,117,117)\", \"font\": \"Helvetica\", \"fontSize\": 11, \"fontWeight\": 300, \"tooltip\": \"\"}, \"encoding\": {\"text\": {\"value\": \"TIMES LARGER\"}, \"x\": {\"value\": 680.0}, \"y\": {\"value\": 7.5}}}, {\"mark\": {\"type\": \"text\", \"align\": \"center\", \"fill\": \"rgb(117,117,117)\", \"font\": \"Helvetica\", \"fontSize\": 11, \"fontWeight\": 300, \"tooltip\": \"\"}, \"encoding\": {\"text\": {\"value\": \"EQUAL\"}, \"x\": {\"value\": 400.0}, \"y\": {\"value\": 7.5}}}], \"data\": {\"name\": \"data-79dd248bdaa0ea02c535240e8ad022c8\"}}, {\"layer\": [{\"mark\": {\"type\": \"rule\", \"opacity\": 0.8, \"stroke\": \"rgb(217, 46, 28)\", \"strokeWidth\": 1.25, \"tooltip\": \"\"}, \"encoding\": {\"x\": {\"type\": \"quantitative\", \"field\": \"min\", \"scale\": {\"domain\": [-2, 2], \"range\": [80.0, 720.0]}}, \"y\": {\"value\": 25.0}, \"y2\": {\"value\": 225.0}}}, {\"mark\": {\"type\": \"rule\", \"opacity\": 0.8, \"stroke\": \"rgb(217, 46, 28)\", \"strokeWidth\": 1.25, \"tooltip\": \"\"}, \"encoding\": {\"x\": {\"type\": \"quantitative\", \"field\": \"max\", \"scale\": {\"domain\": [-2, 2], \"range\": [80.0, 720.0]}}, \"y\": {\"value\": 25.0}, \"y2\": {\"value\": 225.0}}}, {\"layer\": [{\"mark\": {\"type\": \"rect\", \"fill\": \"rgb(217, 46, 28)\", \"opacity\": 0.1, \"tooltip\": \"\"}, \"encoding\": {\"x\": {\"type\": \"quantitative\", \"field\": \"min\", \"scale\": {\"domain\": [-2, 2], \"range\": [80.0, 720.0]}}, \"x2\": {\"field\": \"lower_end\"}, \"y\": {\"value\": 25.0}, \"y2\": {\"value\": 225.0}}}, {\"mark\": {\"type\": \"rect\", \"fill\": \"rgb(217, 46, 28)\", \"opacity\": 0.1, \"tooltip\": \"\"}, \"encoding\": {\"x\": {\"type\": \"quantitative\", \"field\": \"max\", \"scale\": {\"domain\": [-2, 2], \"range\": [80.0, 720.0]}}, \"x2\": {\"field\": \"upper_end\"}, \"y\": {\"value\": 25.0}, \"y2\": {\"value\": 225.0}}}]}, {\"data\": {\"name\": \"data-79dd248bdaa0ea02c535240e8ad022c8\"}, \"mark\": {\"type\": \"text\", \"align\": \"left\", \"baseline\": \"top\", \"fill\": \"rgb(217, 46, 28)\", \"font\": \"Helvetica\", \"fontSize\": 11, \"fontWeight\": 300, \"tooltip\": \"\"}, \"encoding\": {\"text\": {\"value\": \"The metric value for any group should not be 1.3 (or more) times smaller or larger than that of the reference group suburban_rural.\"}, \"x\": {\"value\": 0}, \"y\": {\"value\": 233.33333333333334}}}], \"data\": {\"name\": \"data-b643063563633c31a45d634da5f365b2\"}}, {\"layer\": [{\"mark\": \"circle\"}, {\"data\": {\"name\": \"data-841de853da7588b75c0e669abab68368\"}, \"mark\": {\"type\": \"circle\", \"opacity\": 0.2}, \"encoding\": {\"color\": {\"condition\": {\"type\": \"nominal\", \"field\": \"attribute_value\", \"legend\": null, \"scale\": {\"domain\": [\"suburban_rural\", \"unknown\", \"urban\"], \"range\": [\"rgb(117,117,117)\", \"rgb(100, 143, 255)\", \"rgb(254, 97, 0)\", \"rgb(220, 38, 127)\", \"rgb(255, 176, 0)\", \"rgb(120, 94, 240)\"]}, \"selection\": \"selector005\"}, \"value\": \"rgb(224,224,224)\"}, \"size\": {\"type\": \"quantitative\", \"field\": \"group_size\", \"legend\": null, \"scale\": {\"domain\": [0, 10938.0], \"range\": [0, 7853.981633974483]}}, \"tooltip\": [{\"type\": \"nominal\", \"field\": \"attribute_value\", \"title\": \"Group\"}, {\"type\": \"nominal\", \"field\": \"tooltip_group_size\", \"title\": \"Group Size\"}, {\"type\": \"nominal\", \"field\": \"tooltip_disparity_explanation_tpr\", \"title\": \"Disparity\"}, {\"type\": \"quantitative\", \"field\": \"tpr\", \"format\": \".2f\", \"title\": \"TPR\"}], \"x\": {\"type\": \"quantitative\", \"axis\": {\"domain\": false, \"labels\": false, \"ticks\": false, \"title\": null, \"values\": [-2, -1, 1, 2]}, \"field\": \"tpr_disparity_scaled\", \"scale\": {\"domain\": [-2, 2], \"range\": [80.0, 720.0]}}, \"y\": {\"type\": \"nominal\", \"axis\": {\"domain\": false, \"grid\": false, \"labels\": false, \"ticks\": false, \"title\": \" \"}, \"field\": \"metric_variable\", \"scale\": {\"domain\": [\"TPR\"], \"range\": [30, 225.0]}}}, \"selection\": {\"selector007\": {\"type\": \"multi\", \"empty\": \"all\", \"fields\": [\"attribute_value\"]}}, \"transform\": [{\"calculate\": \"'TPR'\", \"as\": \"metric_variable\"}]}, {\"layer\": [{\"mark\": \"point\"}, {\"data\": {\"name\": \"data-841de853da7588b75c0e669abab68368\"}, \"mark\": {\"type\": \"point\", \"filled\": true, \"size\": 60}, \"encoding\": {\"color\": {\"condition\": {\"type\": \"nominal\", \"field\": \"attribute_value\", \"legend\": null, \"scale\": {\"domain\": [\"suburban_rural\", \"unknown\", \"urban\"], \"range\": [\"rgb(117,117,117)\", \"rgb(100, 143, 255)\", \"rgb(254, 97, 0)\", \"rgb(220, 38, 127)\", \"rgb(255, 176, 0)\", \"rgb(120, 94, 240)\"]}, \"selection\": \"selector005\"}, \"value\": \"rgb(224,224,224)\"}, \"shape\": {\"type\": \"nominal\", \"field\": \"attribute_value\", \"legend\": null, \"scale\": {\"domain\": [\"suburban_rural\", \"unknown\", \"urban\"], \"range\": [\"cross\", \"circle\", \"circle\"]}}, \"tooltip\": [{\"type\": \"nominal\", \"field\": \"attribute_value\", \"title\": \"Group\"}, {\"type\": \"nominal\", \"field\": \"tooltip_group_size\", \"title\": \"Group Size\"}, {\"type\": \"nominal\", \"field\": \"tooltip_disparity_explanation_tpr\", \"title\": \"Disparity\"}, {\"type\": \"quantitative\", \"field\": \"tpr\", \"format\": \".2f\", \"title\": \"TPR\"}], \"x\": {\"type\": \"quantitative\", \"axis\": {\"domain\": false, \"labels\": false, \"ticks\": false, \"title\": null, \"values\": [-2, -1, 1, 2]}, \"field\": \"tpr_disparity_scaled\", \"scale\": {\"domain\": [-2, 2], \"range\": [80.0, 720.0]}}, \"y\": {\"type\": \"nominal\", \"axis\": {\"domain\": false, \"grid\": false, \"labels\": false, \"ticks\": false, \"title\": \" \"}, \"field\": \"metric_variable\", \"scale\": {\"domain\": [\"TPR\"], \"range\": [30, 225.0]}}}, \"selection\": {\"selector006\": {\"type\": \"multi\", \"empty\": \"all\", \"fields\": [\"attribute_value\"]}}, \"transform\": [{\"calculate\": \"'TPR'\", \"as\": \"metric_variable\"}]}]}]}, {\"layer\": [{\"mark\": {\"type\": \"point\", \"filled\": true, \"tooltip\": \"\"}, \"encoding\": {\"color\": {\"condition\": {\"type\": \"nominal\", \"field\": \"attribute_value\", \"legend\": null, \"scale\": {\"domain\": [\"suburban_rural\", \"unknown\", \"urban\"], \"range\": [\"rgb(117,117,117)\", \"rgb(100, 143, 255)\", \"rgb(254, 97, 0)\", \"rgb(220, 38, 127)\", \"rgb(255, 176, 0)\", \"rgb(120, 94, 240)\"]}, \"selection\": \"selector005\"}, \"value\": \"rgb(224,224,224)\"}, \"shape\": {\"type\": \"nominal\", \"field\": \"attribute_value\", \"legend\": null, \"scale\": {\"domain\": [\"suburban_rural\", \"unknown\", \"urban\"], \"range\": [\"cross\", \"circle\", \"circle\"]}}, \"x\": {\"value\": 880.0000000000001}, \"y\": {\"type\": \"nominal\", \"axis\": {\"domain\": false, \"grid\": false, \"labels\": false, \"ticks\": false, \"title\": \" \"}, \"field\": \"attribute_value\", \"scale\": {\"domain\": [\"suburban_rural\", \"unknown\", \"urban\"], \"range\": [21, 94]}}}, \"selection\": {\"selector005\": {\"type\": \"multi\", \"empty\": \"all\", \"fields\": [\"attribute_value\"]}}}, {\"mark\": {\"type\": \"text\", \"align\": \"left\", \"baseline\": \"middle\", \"font\": \"Helvetica\", \"fontSize\": 11, \"fontWeight\": 300, \"tooltip\": \"\"}, \"encoding\": {\"color\": {\"condition\": {\"type\": \"nominal\", \"field\": \"attribute_value\", \"legend\": null, \"scale\": {\"domain\": [\"suburban_rural\", \"unknown\", \"urban\"], \"range\": [\"rgb(117,117,117)\", \"rgb(100, 143, 255)\", \"rgb(254, 97, 0)\", \"rgb(220, 38, 127)\", \"rgb(255, 176, 0)\", \"rgb(120, 94, 240)\"]}, \"selection\": \"selector005\"}, \"value\": \"rgb(224,224,224)\"}, \"text\": {\"type\": \"nominal\", \"field\": \"label\"}, \"x\": {\"value\": 890.0000000000001}, \"y\": {\"type\": \"nominal\", \"axis\": {\"domain\": false, \"grid\": false, \"labels\": false, \"ticks\": false, \"title\": \" \"}, \"field\": \"attribute_value\", \"scale\": {\"domain\": [\"suburban_rural\", \"unknown\", \"urban\"], \"range\": [21, 94]}}}, \"selection\": {\"selector008\": {\"type\": \"multi\", \"empty\": \"all\", \"fields\": [\"attribute_value\"]}}}, {\"data\": {\"name\": \"data-79dd248bdaa0ea02c535240e8ad022c8\"}, \"mark\": {\"type\": \"text\", \"align\": \"left\", \"baseline\": \"middle\", \"color\": \"rgb(117,117,117)\", \"font\": \"Helvetica\", \"fontSize\": 11, \"fontWeight\": 300, \"tooltip\": \"\"}, \"encoding\": {\"text\": {\"value\": \"Click to highlight a group.\"}, \"x\": {\"value\": 880.0000000000001}, \"y\": {\"value\": 0}}}], \"data\": {\"name\": \"data-e1f9423a4bded23d71327d85bde107c5\"}}], \"height\": 250, \"resolve\": {\"scale\": {\"size\": \"independent\", \"y\": \"independent\"}}, \"width\": 800, \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.8.1.json\", \"datasets\": {\"data-b3bcfd1decd67fb31f95ea5e1bcd9e64\": [{\"metric\": \"TPR\", \"x\": -2.2, \"x2\": 2.2}], \"data-79dd248bdaa0ea02c535240e8ad022c8\": [{\"a\": 1, \"b\": 0}, {\"a\": 1, \"b\": 0}], \"data-bf3bbf11504879c3d6556966f4853596\": [{\"value\": -2, \"label\": 3}, {\"value\": -1, \"label\": 2}, {\"value\": 0, \"label\": \"=\"}, {\"value\": 1, \"label\": 2}, {\"value\": 2, \"label\": 3}], \"data-b643063563633c31a45d634da5f365b2\": [{\"min\": -0.30000000000000004, \"max\": 0.30000000000000004, \"lower_end\": -2.2, \"upper_end\": 2.2}], \"data-841de853da7588b75c0e669abab68368\": [{\"attribute_name\": \"metro_type\", \"attribute_value\": \"suburban_rural\", \"group_size\": 6506, \"total_entities\": 17677, \"tpr\": 0.13079816130380276, \"tpr_disparity\": 1.0, \"tpr_disparity_scaled\": 0.0, \"tooltip_group_size\": \"6,506 (36.80%)\", \"tooltip_disparity_explanation_tpr\": \"Reference group\"}, {\"attribute_name\": \"metro_type\", \"attribute_value\": \"unknown\", \"group_size\": 2056, \"total_entities\": 17677, \"tpr\": 0.12549019607843137, \"tpr_disparity\": 0.9594186556411702, \"tpr_disparity_scaled\": -0.042297847889678275, \"tooltip_group_size\": \"2,056 (11.63%)\", \"tooltip_disparity_explanation_tpr\": \"1.04 times smaller TPR than the reference group\"}, {\"attribute_name\": \"metro_type\", \"attribute_value\": \"urban\", \"group_size\": 9115, \"total_entities\": 17677, \"tpr\": 0.04907539118065434, \"tpr_disparity\": 0.37519939647062567, \"tpr_disparity_scaled\": -1.665249489755749, \"tooltip_group_size\": \"9,115 (51.56%)\", \"tooltip_disparity_explanation_tpr\": \"2.67 times smaller TPR than the reference group\"}], \"data-e1f9423a4bded23d71327d85bde107c5\": [{\"attribute_value\": \"suburban_rural\", \"label\": \"suburban_rural [REF]\"}, {\"attribute_value\": \"unknown\", \"label\": \"unknown\"}, {\"attribute_value\": \"urban\", \"label\": \"urban\"}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.LayerChart(...)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ap.disparities(bdf, metrics, 'metro_type', fairness_threshold = 1.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div id=\"altair-viz-0334d0ca3a664e12b504421c1fd92828\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-0334d0ca3a664e12b504421c1fd92828\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-0334d0ca3a664e12b504421c1fd92828\");\n",
       "    }\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.8.1?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function loadScript(lib) {\n",
       "      return new Promise(function(resolve, reject) {\n",
       "        var s = document.createElement('script');\n",
       "        s.src = paths[lib];\n",
       "        s.async = true;\n",
       "        s.onload = () => resolve(paths[lib]);\n",
       "        s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "        document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "      });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else if (typeof vegaEmbed === \"function\") {\n",
       "      displayChart(vegaEmbed);\n",
       "    } else {\n",
       "      loadScript(\"vega\")\n",
       "        .then(() => loadScript(\"vega-lite\"))\n",
       "        .then(() => loadScript(\"vega-embed\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300, \"strokeWidth\": 0}, \"axisLeft\": {\"labelColor\": \"rgb(117,117,117)\", \"labelFont\": \"Helvetica\", \"labelFontSize\": 16}}, \"layer\": [{\"data\": {\"name\": \"data-a80bbe78875d74850191493116348974\"}, \"mark\": {\"type\": \"rule\", \"stroke\": \"rgb(117,117,117)\", \"strokeWidth\": 1.25, \"tooltip\": \"\"}, \"encoding\": {\"x\": {\"type\": \"quantitative\", \"field\": \"x\", \"scale\": {\"domain\": [-3, 3], \"range\": [80.0, 720.0]}}, \"x2\": {\"field\": \"x2\"}, \"y\": {\"type\": \"nominal\", \"axis\": {\"domain\": false, \"labelAngle\": 0, \"labelPadding\": -30, \"orient\": \"left\", \"ticks\": false, \"title\": \"\"}, \"field\": \"metric\", \"scale\": {\"domain\": [\"TPR\"], \"range\": [30, 225.0]}}}}, {\"data\": {\"name\": \"data-79dd248bdaa0ea02c535240e8ad022c8\"}, \"mark\": {\"type\": \"rule\", \"stroke\": \"rgb(117,117,117)\", \"strokeDash\": [5, 5], \"strokeWidth\": 1}, \"encoding\": {\"tooltip\": {\"value\": \"male [REF]\"}, \"x\": {\"value\": 400.0}, \"y\": {\"value\": 20.833333333333336}, \"y2\": {\"value\": 229.16666666666666}}}, {\"data\": {\"name\": \"data-f70f876ef858e7578abafe4eee75fee4\"}, \"mark\": {\"type\": \"text\", \"align\": \"center\", \"color\": \"rgb(117,117,117)\", \"font\": \"Helvetica\", \"fontSize\": 11, \"fontWeight\": 300, \"tooltip\": \"\"}, \"encoding\": {\"text\": {\"type\": \"nominal\", \"field\": \"label\"}, \"x\": {\"type\": \"quantitative\", \"field\": \"value\", \"scale\": {\"domain\": [-3, 3], \"range\": [80.0, 720.0]}}, \"y\": {\"value\": 17.5}}}, {\"layer\": [{\"mark\": {\"type\": \"text\", \"align\": \"right\", \"fill\": \"rgb(117,117,117)\", \"font\": \"Helvetica\", \"fontSize\": 11, \"fontWeight\": 300, \"tooltip\": \"\"}, \"encoding\": {\"text\": {\"value\": \"TIMES SMALLER\"}, \"x\": {\"value\": 120.0}, \"y\": {\"value\": 7.5}}}, {\"mark\": {\"type\": \"text\", \"align\": \"left\", \"fill\": \"rgb(117,117,117)\", \"font\": \"Helvetica\", \"fontSize\": 11, \"fontWeight\": 300, \"tooltip\": \"\"}, \"encoding\": {\"text\": {\"value\": \"TIMES LARGER\"}, \"x\": {\"value\": 680.0}, \"y\": {\"value\": 7.5}}}, {\"mark\": {\"type\": \"text\", \"align\": \"center\", \"fill\": \"rgb(117,117,117)\", \"font\": \"Helvetica\", \"fontSize\": 11, \"fontWeight\": 300, \"tooltip\": \"\"}, \"encoding\": {\"text\": {\"value\": \"EQUAL\"}, \"x\": {\"value\": 400.0}, \"y\": {\"value\": 7.5}}}], \"data\": {\"name\": \"data-79dd248bdaa0ea02c535240e8ad022c8\"}}, {\"layer\": [{\"mark\": {\"type\": \"rule\", \"opacity\": 0.8, \"stroke\": \"rgb(217, 46, 28)\", \"strokeWidth\": 1.25, \"tooltip\": \"\"}, \"encoding\": {\"x\": {\"type\": \"quantitative\", \"field\": \"min\", \"scale\": {\"domain\": [-3, 3], \"range\": [80.0, 720.0]}}, \"y\": {\"value\": 25.0}, \"y2\": {\"value\": 225.0}}}, {\"mark\": {\"type\": \"rule\", \"opacity\": 0.8, \"stroke\": \"rgb(217, 46, 28)\", \"strokeWidth\": 1.25, \"tooltip\": \"\"}, \"encoding\": {\"x\": {\"type\": \"quantitative\", \"field\": \"max\", \"scale\": {\"domain\": [-3, 3], \"range\": [80.0, 720.0]}}, \"y\": {\"value\": 25.0}, \"y2\": {\"value\": 225.0}}}, {\"layer\": [{\"mark\": {\"type\": \"rect\", \"fill\": \"rgb(217, 46, 28)\", \"opacity\": 0.1, \"tooltip\": \"\"}, \"encoding\": {\"x\": {\"type\": \"quantitative\", \"field\": \"min\", \"scale\": {\"domain\": [-3, 3], \"range\": [80.0, 720.0]}}, \"x2\": {\"field\": \"lower_end\"}, \"y\": {\"value\": 25.0}, \"y2\": {\"value\": 225.0}}}, {\"mark\": {\"type\": \"rect\", \"fill\": \"rgb(217, 46, 28)\", \"opacity\": 0.1, \"tooltip\": \"\"}, \"encoding\": {\"x\": {\"type\": \"quantitative\", \"field\": \"max\", \"scale\": {\"domain\": [-3, 3], \"range\": [80.0, 720.0]}}, \"x2\": {\"field\": \"upper_end\"}, \"y\": {\"value\": 25.0}, \"y2\": {\"value\": 225.0}}}]}, {\"data\": {\"name\": \"data-79dd248bdaa0ea02c535240e8ad022c8\"}, \"mark\": {\"type\": \"text\", \"align\": \"left\", \"baseline\": \"top\", \"fill\": \"rgb(217, 46, 28)\", \"font\": \"Helvetica\", \"fontSize\": 11, \"fontWeight\": 300, \"tooltip\": \"\"}, \"encoding\": {\"text\": {\"value\": \"The metric value for any group should not be 1.3 (or more) times smaller or larger than that of the reference group male.\"}, \"x\": {\"value\": 0}, \"y\": {\"value\": 233.33333333333334}}}], \"data\": {\"name\": \"data-3dee117d57e1ece1df7853af976ed45d\"}}, {\"layer\": [{\"mark\": \"circle\"}, {\"data\": {\"name\": \"data-91809a955570d5db1b2a88524ff9929a\"}, \"mark\": {\"type\": \"circle\", \"opacity\": 0.2}, \"encoding\": {\"color\": {\"condition\": {\"type\": \"nominal\", \"field\": \"attribute_value\", \"legend\": null, \"scale\": {\"domain\": [\"male\", \"female\"], \"range\": [\"rgb(117,117,117)\", \"rgb(100, 143, 255)\", \"rgb(254, 97, 0)\", \"rgb(220, 38, 127)\", \"rgb(255, 176, 0)\", \"rgb(120, 94, 240)\"]}, \"selection\": \"selector009\"}, \"value\": \"rgb(224,224,224)\"}, \"size\": {\"type\": \"quantitative\", \"field\": \"group_size\", \"legend\": null, \"scale\": {\"domain\": [0, 18111.6], \"range\": [0, 7853.981633974483]}}, \"tooltip\": [{\"type\": \"nominal\", \"field\": \"attribute_value\", \"title\": \"Group\"}, {\"type\": \"nominal\", \"field\": \"tooltip_group_size\", \"title\": \"Group Size\"}, {\"type\": \"nominal\", \"field\": \"tooltip_disparity_explanation_tpr\", \"title\": \"Disparity\"}, {\"type\": \"quantitative\", \"field\": \"tpr\", \"format\": \".2f\", \"title\": \"TPR\"}], \"x\": {\"type\": \"quantitative\", \"axis\": {\"domain\": false, \"labels\": false, \"ticks\": false, \"title\": null, \"values\": [-3, -2, -1, 1, 2, 3]}, \"field\": \"tpr_disparity_scaled\", \"scale\": {\"domain\": [-3, 3], \"range\": [80.0, 720.0]}}, \"y\": {\"type\": \"nominal\", \"axis\": {\"domain\": false, \"grid\": false, \"labels\": false, \"ticks\": false, \"title\": \" \"}, \"field\": \"metric_variable\", \"scale\": {\"domain\": [\"TPR\"], \"range\": [30, 225.0]}}}, \"selection\": {\"selector011\": {\"type\": \"multi\", \"empty\": \"all\", \"fields\": [\"attribute_value\"]}}, \"transform\": [{\"calculate\": \"'TPR'\", \"as\": \"metric_variable\"}]}, {\"layer\": [{\"mark\": \"point\"}, {\"data\": {\"name\": \"data-91809a955570d5db1b2a88524ff9929a\"}, \"mark\": {\"type\": \"point\", \"filled\": true, \"size\": 60}, \"encoding\": {\"color\": {\"condition\": {\"type\": \"nominal\", \"field\": \"attribute_value\", \"legend\": null, \"scale\": {\"domain\": [\"male\", \"female\"], \"range\": [\"rgb(117,117,117)\", \"rgb(100, 143, 255)\", \"rgb(254, 97, 0)\", \"rgb(220, 38, 127)\", \"rgb(255, 176, 0)\", \"rgb(120, 94, 240)\"]}, \"selection\": \"selector009\"}, \"value\": \"rgb(224,224,224)\"}, \"shape\": {\"type\": \"nominal\", \"field\": \"attribute_value\", \"legend\": null, \"scale\": {\"domain\": [\"male\", \"female\"], \"range\": [\"cross\", \"circle\"]}}, \"tooltip\": [{\"type\": \"nominal\", \"field\": \"attribute_value\", \"title\": \"Group\"}, {\"type\": \"nominal\", \"field\": \"tooltip_group_size\", \"title\": \"Group Size\"}, {\"type\": \"nominal\", \"field\": \"tooltip_disparity_explanation_tpr\", \"title\": \"Disparity\"}, {\"type\": \"quantitative\", \"field\": \"tpr\", \"format\": \".2f\", \"title\": \"TPR\"}], \"x\": {\"type\": \"quantitative\", \"axis\": {\"domain\": false, \"labels\": false, \"ticks\": false, \"title\": null, \"values\": [-3, -2, -1, 1, 2, 3]}, \"field\": \"tpr_disparity_scaled\", \"scale\": {\"domain\": [-3, 3], \"range\": [80.0, 720.0]}}, \"y\": {\"type\": \"nominal\", \"axis\": {\"domain\": false, \"grid\": false, \"labels\": false, \"ticks\": false, \"title\": \" \"}, \"field\": \"metric_variable\", \"scale\": {\"domain\": [\"TPR\"], \"range\": [30, 225.0]}}}, \"selection\": {\"selector010\": {\"type\": \"multi\", \"empty\": \"all\", \"fields\": [\"attribute_value\"]}}, \"transform\": [{\"calculate\": \"'TPR'\", \"as\": \"metric_variable\"}]}]}]}, {\"layer\": [{\"mark\": {\"type\": \"point\", \"filled\": true, \"tooltip\": \"\"}, \"encoding\": {\"color\": {\"condition\": {\"type\": \"nominal\", \"field\": \"attribute_value\", \"legend\": null, \"scale\": {\"domain\": [\"male\", \"female\"], \"range\": [\"rgb(117,117,117)\", \"rgb(100, 143, 255)\", \"rgb(254, 97, 0)\", \"rgb(220, 38, 127)\", \"rgb(255, 176, 0)\", \"rgb(120, 94, 240)\"]}, \"selection\": \"selector009\"}, \"value\": \"rgb(224,224,224)\"}, \"shape\": {\"type\": \"nominal\", \"field\": \"attribute_value\", \"legend\": null, \"scale\": {\"domain\": [\"male\", \"female\"], \"range\": [\"cross\", \"circle\"]}}, \"x\": {\"value\": 880.0000000000001}, \"y\": {\"type\": \"nominal\", \"axis\": {\"domain\": false, \"grid\": false, \"labels\": false, \"ticks\": false, \"title\": \" \"}, \"field\": \"attribute_value\", \"scale\": {\"domain\": [\"male\", \"female\"], \"range\": [21, 73]}}}, \"selection\": {\"selector009\": {\"type\": \"multi\", \"empty\": \"all\", \"fields\": [\"attribute_value\"]}}}, {\"mark\": {\"type\": \"text\", \"align\": \"left\", \"baseline\": \"middle\", \"font\": \"Helvetica\", \"fontSize\": 11, \"fontWeight\": 300, \"tooltip\": \"\"}, \"encoding\": {\"color\": {\"condition\": {\"type\": \"nominal\", \"field\": \"attribute_value\", \"legend\": null, \"scale\": {\"domain\": [\"male\", \"female\"], \"range\": [\"rgb(117,117,117)\", \"rgb(100, 143, 255)\", \"rgb(254, 97, 0)\", \"rgb(220, 38, 127)\", \"rgb(255, 176, 0)\", \"rgb(120, 94, 240)\"]}, \"selection\": \"selector009\"}, \"value\": \"rgb(224,224,224)\"}, \"text\": {\"type\": \"nominal\", \"field\": \"label\"}, \"x\": {\"value\": 890.0000000000001}, \"y\": {\"type\": \"nominal\", \"axis\": {\"domain\": false, \"grid\": false, \"labels\": false, \"ticks\": false, \"title\": \" \"}, \"field\": \"attribute_value\", \"scale\": {\"domain\": [\"male\", \"female\"], \"range\": [21, 73]}}}, \"selection\": {\"selector012\": {\"type\": \"multi\", \"empty\": \"all\", \"fields\": [\"attribute_value\"]}}}, {\"data\": {\"name\": \"data-79dd248bdaa0ea02c535240e8ad022c8\"}, \"mark\": {\"type\": \"text\", \"align\": \"left\", \"baseline\": \"middle\", \"color\": \"rgb(117,117,117)\", \"font\": \"Helvetica\", \"fontSize\": 11, \"fontWeight\": 300, \"tooltip\": \"\"}, \"encoding\": {\"text\": {\"value\": \"Click to highlight a group.\"}, \"x\": {\"value\": 880.0000000000001}, \"y\": {\"value\": 0}}}], \"data\": {\"name\": \"data-b9f75b5c0827c85fba1c9c9d5d5ac8e6\"}}], \"height\": 250, \"resolve\": {\"scale\": {\"size\": \"independent\", \"y\": \"independent\"}}, \"width\": 800, \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.8.1.json\", \"datasets\": {\"data-a80bbe78875d74850191493116348974\": [{\"metric\": \"TPR\", \"x\": -3.2, \"x2\": 3.2}], \"data-79dd248bdaa0ea02c535240e8ad022c8\": [{\"a\": 1, \"b\": 0}, {\"a\": 1, \"b\": 0}], \"data-f70f876ef858e7578abafe4eee75fee4\": [{\"value\": -3, \"label\": 4}, {\"value\": -2, \"label\": 3}, {\"value\": -1, \"label\": 2}, {\"value\": 0, \"label\": \"=\"}, {\"value\": 1, \"label\": 2}, {\"value\": 2, \"label\": 3}, {\"value\": 3, \"label\": 4}], \"data-3dee117d57e1ece1df7853af976ed45d\": [{\"min\": -0.30000000000000004, \"max\": 0.30000000000000004, \"lower_end\": -3.2, \"upper_end\": 3.2}], \"data-91809a955570d5db1b2a88524ff9929a\": [{\"attribute_name\": \"teacher_sex\", \"attribute_value\": \"female\", \"group_size\": 15093, \"total_entities\": 17677, \"tpr\": 0.10102972605401205, \"tpr_disparity\": 3.0795357237945153, \"tpr_disparity_scaled\": 2.0795357237945153, \"tooltip_group_size\": \"15,093 (85.38%)\", \"tooltip_disparity_explanation_tpr\": \"3.08 times larger TPR than the reference group\"}, {\"attribute_name\": \"teacher_sex\", \"attribute_value\": \"male\", \"group_size\": 2584, \"total_entities\": 17677, \"tpr\": 0.032806804374240585, \"tpr_disparity\": 1.0, \"tpr_disparity_scaled\": 0.0, \"tooltip_group_size\": \"2,584 (14.62%)\", \"tooltip_disparity_explanation_tpr\": \"Reference group\"}], \"data-b9f75b5c0827c85fba1c9c9d5d5ac8e6\": [{\"attribute_value\": \"male\", \"label\": \"male [REF]\"}, {\"attribute_value\": \"female\", \"label\": \"female\"}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.LayerChart(...)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ap.disparities(bdf, metrics, 'teacher_sex', fairness_threshold = 1.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can also look at the raw metrics to see what's changed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "absolute_metrics = g.list_absolute_metrics(xtab)\n",
    "xtab[['attribute_name', 'attribute_value'] + absolute_metrics]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding to the model selection (tradeoff) graph\n",
    "\n",
    "Finally, let's look at how this new option stacks up against what we plotted in our model selection process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mitigated_precision = df.loc[df['score']==1]['label_value'].mean()\n",
    "plot_configs = {\n",
    "    'evals_df':evals_df, \n",
    "    'aequitas_df':aequitas_df,\n",
    "    'attr_col':'poverty_level', \n",
    "    'group_name':'highest',\n",
    "    'performance_col':'model_precision',\n",
    "    'bias_metric':'tpr', \n",
    "    'flip_disparity':True, \n",
    "    'mitigated_tag':'Resampling',\n",
    "    'mitigated_bdf':bdf, \n",
    "    'mitigated_performance':mitigated_precision, \n",
    "    'ylim':None\n",
    "}\n",
    "create_scatter_disparity_performance(**plot_configs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=red>Bias Reduction Strategy 2: Regularization (using Fairlearn package)</font>\n",
    "### <font color=red>In-Processing Fairness Improvement</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Install dependencies, import packages and data\n",
    "This is needed every time you open this notebook in colab to install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's use the methods \n",
    "from fairlearn.reductions import ExponentiatedGradient, GridSearch, DemographicParity, TruePositiveRateDifference\n",
    "from fairlearn.metrics import selection_rate_group_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What has already happened?\n",
    "\n",
    "We've already cleaned data, generated features, created train-test sets, built 1000s of models on each training set and scored each test set with them, and calculated various evaluation metrics. We then used these results to pick a \"best\" model in terms of performance on the \"accuracy\" metric we care about: **Precision at the top 1000** (corresponding to our goal of selecting 1000 project submissions that are most likely to not get funded in order to prioritize resource allocation).\n",
    "\n",
    "When we audited this selected model with Aequitas, however, we found biases across many attributes, including the poverty level of the schools. Here, we explore a method of using in-processing to train a fairness-aware classifier in order to reduce this bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=green>FairLearn - a reductions approach</font>\n",
    "\n",
    "[Paper](https://arxiv.org/pdf/1803.02453.pdf): _A Reductions Approach to Fair Classification_, 2018\n",
    "\n",
    "> We present a systematic approach for achievingfairness in a binary classification setting. Whilewe focus on two well-known quantitative defini-tions of fairness, our approach encompasses manyother  previously  studied  definitions  as  specialcases. The key idea is to __reduce fair classification__ to a __sequence  of  cost-sensitive__  classification problems, whose solutions yield a randomized classifier with the __lowest (empirical) error__ subject to  the  __desired  constraints__.   We  introduce  two reductions that work for any representation of the cost-sensitive  classifier  and  compare  favorably to prior baselines on a variety of data sets, while overcoming several of their disadvantages.\n",
    "\n",
    "[FairLearn Documentation](https://fairlearn.github.io/user_guide/mitigation.html#id17)\n",
    "\n",
    "\n",
    "\n",
    "### TLDR; \n",
    "\n",
    "- This approach poses Fair Learning as a constrained optimization problem: minimize the empirical error, subject to linear constraints of the fairness (e.g., TPR difference, demographic parity).\n",
    "- Solve the constrained optimization as a __cost-sensitive__ classification problem.\n",
    "- Obtain a __randomized classifier__, which implies they will create multiple base estimators.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load train and test matrices as well as protected attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindf = pd.read_csv(DATAPATH + 'train_20120501_20120801.csv.gz', compression='gzip')\n",
    "testdf = pd.read_csv(DATAPATH + 'test_20121201_20130201.csv.gz', compression='gzip')\n",
    "train_attrdf = pd.read_csv(DATAPATH + 'train_20120501_20120801_protected.csv.gz', compression='gzip')\n",
    "test_attrdf = pd.read_csv(DATAPATH + 'test_20121201_20130201_protected.csv.gz', compression='gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up some parameters we'll need below and create matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_col = 'quickstart_label'\n",
    "date_col = 'as_of_date'\n",
    "id_col = 'entity_id'\n",
    "attr_col = 'poverty_level'\n",
    "exclude_cols = [label_col, date_col, id_col]\n",
    "\n",
    "top_k = 1000\n",
    "\n",
    "# aequitas parameters\n",
    "metrics = ['tpr']\n",
    "disparity_threshold = 1.3\n",
    "protected_attribute_ref_group = {attr_col:'lower'}\n",
    "\n",
    "\n",
    "X_train, y_train, A_train = traindf[[c for c in traindf.columns if c not in exclude_cols]].values, traindf[label_col].values, train_attrdf[[attr_col]]\n",
    "X_test,   y_test,   A_test   = testdf[[c for c in testdf.columns if c not in exclude_cols]].values,   testdf[label_col].values  , test_attrdf[[attr_col]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up a fairness-improving classifier\n",
    "\n",
    "To account fairness during model training, we'll use the **Exponentiated Gradient** provided by the `fairlearn` module.\n",
    "\n",
    "\n",
    "Its hyperparameters are: \n",
    "- `estimator`: an estimator that implements the methods `fit(X, y, sample_weight)` and `predict(X)`.\n",
    "- `constraints`: fairness constraints.\n",
    "- `eps: float`: fairness threshold, i.e., how much constraint violation we support (defaults to 0.01). \n",
    "- `T: int`: maximum number of iterations (defaults to 50).\n",
    "- `nu: float`: convergence threshold for duality gap (defaults to None).\n",
    "- `eta_0: float`: initial learning rate (defaults to 2).\n",
    "- `run_linprog_step: bool`: whether to apply saddle point optimization to the convex hull of classifiers obtained so far, after each exponentiated gradient step (defaults to True)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: Exponentiated Gradient has a stoachastic component\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we're using `TruePositiveRateDifference` for our fairness constraint here since we care about equalizing **recall** (aka **tpr** aka **equality of opportunity**) across our subgroups:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Step 1. Define the constraint\n",
    "constraint = TruePositiveRateDifference()\n",
    "\n",
    "# Step 2. Define the base estimator (any estimator providing 'fit' and 'predict')\n",
    "# Note: we could have used other algorithm such as logistic regression or random forest\n",
    "base_estimator = DecisionTreeClassifier(max_depth=20, min_samples_leaf=10)\n",
    "\n",
    "# Step 3. Define the bias reducer algorithm you want to apply\n",
    "bias_reducer = ExponentiatedGradient(base_estimator, constraint, T=50)\n",
    "\n",
    "# Step 4. Fit the data (and provide the sensitive attributes)\n",
    "bias_reducer.fit(X_train, y_train, sensitive_features=A_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict on our test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5. Use the mitigator to make predictions \n",
    "y_pred = bias_reducer.predict(X_test)\n",
    "new_preds = testdf[['entity_id','as_of_date','quickstart_label']].copy()\n",
    "new_preds['score'] = y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at the output\n",
    "Notice that unlike many classifiers, the `ExponentiatedGradient` doesn't have a method for predicting a continuous score, just predicted classes of 0 or 1. How many projects did this model predict are at risk of going unfunded (that is, predicted class of 1)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_preds['score'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like about 6,500 projects are predicted as being at risk of going unfunded by this classifier, but unfortunately our program is resource-constrained and can only help 1,000 of them. At this point, if we wanted to pick out the 1,000 highest-risk projects (subject to our fairness constraint), we're a bit stuck: **the classifier doesn't give us any method for distinguishing higher-risk vs lower-risk projects!**\n",
    "\n",
    "Given this limitation, we might posit that a reasonable approach would be to pick 1,000 projects to intervene with from among these 6,500. Let's see what would happen if we did that..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Before and After: Run Aequitas\n",
    "\n",
    "For reference, let's start with looking at the \"best\" model we chose in terms of overall precision at 1,000:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the predictions from the \"best\" model chosen earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_preds = pd.read_csv(DATAPATH + 'predictions_c598fbe93f4c218ac7d325fb478598f1.csv.gz', compression='gzip')\n",
    "old_attrdf = pd.read_csv(DATAPATH + 'test_20121201_20130201_protected.csv.gz', compression='gzip')\n",
    "\n",
    "old_df = pd.merge(old_preds, old_attrdf, how='left', on=[id_col, date_col], left_index=True, right_index=False, sort=True, copy=True)\n",
    "\n",
    "old_df = old_df.sort_values('predict_proba', ascending=False)\n",
    "old_df = old_df.rename(columns = {label_col:'label_value'}) # naming for Aequitas\n",
    "\n",
    "# create a \"score\" column with the predicted class (named \"score\" for use with Aequitas below)\n",
    "old_df['score'] = old_df.apply(lambda x: 1.0 if x.name in old_df.head(top_k).index.tolist() else 0, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Before: Run Aequitas for the \"best\" model chosen earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = Group()\n",
    "b = Bias()\n",
    "\n",
    "xtab_old, _ = g.get_crosstabs(old_df[['score', 'label_value', attr_col]].copy())\n",
    "bdf_old = b.get_disparity_predefined_groups(xtab_old, original_df=old_df, ref_groups_dict=protected_attribute_ref_group)\n",
    "\n",
    "ap.disparities_metrics(bdf_old, metrics, attr_col, fairness_threshold=disparity_threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### After: Run Aequitas for the new, fairness-aware model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember here that we would choose 1,000 at random from the 6,500 with predicted class 1, so the expected value for the recall disparity of this randomly-selected set would just be the value of full set (that is, the recall of each subgroup would, on average, be proportionally lower in the sub-sample):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(new_preds, test_attrdf, how='left', on=['entity_id','as_of_date'], left_index=True, right_index=False, sort=True, copy=True)\n",
    "df = df.rename(columns = {label_col:'label_value'})\n",
    "\n",
    "g = Group()\n",
    "b = Bias()\n",
    "\n",
    "xtab, _ = g.get_crosstabs(df[['score','label_value',attr_col]].copy())\n",
    "bdf = b.get_disparity_predefined_groups(xtab, original_df=df, ref_groups_dict=protected_attribute_ref_group)\n",
    "\n",
    "ap.disparities(bdf, metrics, attr_col, fairness_threshold=disparity_threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That looks pretty good! The new model appears to have reduced the disparity across poverty levels considerably relative to what we saw when training a model without a fairness constraint and choosing based on precision alone.\n",
    "\n",
    "However, the natural question here is where there is a fairness-accuracy trade-off here: What cost did we incur in terms of model performance, that is overall precision?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Precision of the \"best\" model chosen earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_df.loc[old_df['score']==1]['label_value'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Precision of the new, fairness-aware model\n",
    "\n",
    "As above, we would be sampling from the 6,500 down to 1,000 but the expected value of precision in this sample would just be the mean label value in the full population:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['score']==1]['label_value'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, compared to the old model, this method has resulted in **quite a large trade-off in model performance to acheive fairness**. This is certainly in part a result of the lack of flexibility in the method not allowing us to provide a score threshold or top k size that we're interested in equalizing our fairness metric around and instead using a built-in threshold that yields 6,500 predicted positives when we're only able to intervene on 1,000. Unfortunately, this sort of inflexibility appears to be a common attribute of many in-processing methods available today.\n",
    "\n",
    "For context, the overall base rate (`df['label_value'].mean()`) is 0.338, so the drop-off in precision here is about half way from our previous model to simply choosing at random."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding to model selection\n",
    "\n",
    "Finally, let's look at how this new option stacks up against what we plotted in our model selection process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mitigated_precision = df.loc[mitigated_df['score']==1]['label_value'].mean()\n",
    "plot_configs = {\n",
    "    'evals_df':evals_df, \n",
    "    'aequitas_df':aequitas_df,\n",
    "    'attr_col':'poverty_level', \n",
    "    'group_name':'highest',\n",
    "    'performance_col':'model_precision',\n",
    "    'bias_metric':'tpr', \n",
    "    'flip_disparity':True, \n",
    "    'mitigated_tag':'Fairlearn Reductions',\n",
    "    'mitigated_bdf':bdf, \n",
    "    'mitigated_performance':mitigated_precision, \n",
    "    'ylim':None\n",
    "}\n",
    "create_scatter_disparity_performance(**plot_configs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=red>Bias Reduction Strategy 3: Post-Hoc Disparity Mitigation</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What has already happened?\n",
    "\n",
    "We've already cleaned data, generated features, created train-test sets, built 1000s of models on each training set and scored each test set with them, and calculated various evaluation metrics. We then used these results to pick a \"best\" model in terms of performance on the \"accuracy\" metric we care about: **Precision at the top 1000** (corresponding to our goal of selecting 1000 project submissions that are most likely to not get funded in order to prioritize resource allocation).\n",
    "\n",
    "When we audited this selected model with Aequitas, however, we found biases across many attributes, including the poverty level of the schools. Here, we explore a method of using post-hoc disparity mitigation to reduce this bias in the selected model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=green>Intro to Post-Hoc Bias Mitigation</font>\n",
    "\n",
    "![Diagram of Post-Hoc Adjustments](post_hoc_adj.png \"Post-Hoc Adjustments\")\n",
    "\n",
    "One approach to improving the fairness of our model is to make post-hoc adjustments to the thresholds used for each subgroup to choose the 1,000 projects on which to intervene. Because our fairness metric here (**recall** aka **tpr** aka **equality of opportunity**) is monatonically increasing with the depth of the score, we should be able to find score thresholds for each subgroup that will equalize this metric across the groups, subject to the constraint that we want to choose a total of 1,000 projects for our intervation.\n",
    "\n",
    "In short, here's how this will work (see the references below for a more detailed discussion):\n",
    "1. Train the model as usual on a training set, predict scores on a test set\n",
    "2. Split this test set by subgroups on our protected attribute (here, poverty level)\n",
    "3. Sort each subgroup by score and calculate the cumulative tpr/recall up to and including each row in the set, storing this \"rolling within-subgroup recall\" value\n",
    "4. Recombine the subgroups, and sort the entire set by this new value\n",
    "5. Take the top 1,000 projects from this re-ordered list and use it to calculate \"top k\" sizes for each subgroup that equalize recall\n",
    "6. Then, on a future test set, use these calculated subgroup list sizes to assess the impact of disparities and overall precision\n",
    "\n",
    "References:\n",
    "- Hardt, et al, [Equality of Opportunity in Supervised Learning](http://papers.nips.cc/paper/6373-equality-of-opportunity-in-supervised-learning)\n",
    "- Rodolfa, et al, [Case Study: Predictive Fairness to Reduce Misdemeanor Recidivism Through Social Service Interventions](https://dl.acm.org/doi/abs/10.1145/3351095.3372863?casa_token=zc196JJrqkkAAAAA:bPmqmKrA91esJhIxHPT4K1crWWb5JGcflVFDkTgODctMzLpUX50_56Kyyh4NJ2GTd_QSydqhNpjT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the train, test, and protected attributes from the first split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split1_traindf = pd.read_csv(DATAPATH + 'train_20111101_20120201.csv.gz', compression='gzip')\n",
    "split1_testdf = pd.read_csv(DATAPATH + 'test_20120601_20120801.csv.gz', compression='gzip')\n",
    "split1_attrdf = pd.read_csv(DATAPATH + 'test_20120601_20120801_protected.csv.gz', compression='gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's just take a quick look at the data to make sure it makes sense:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split1_traindf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up some parameters we'll need below\n",
    "\n",
    "Note that the classifier type and hyperparameters here are from the best-performing model we chose above based on precision on the top 1000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    'criterion': 'gini',\n",
    "    'max_depth': 30,\n",
    "    'max_features': 'sqrt',\n",
    "    'min_samples_leaf': 44,\n",
    "    'min_samples_split': 3,\n",
    "    'n_estimators': 87,\n",
    "    'n_jobs': -1,\n",
    "    'random_state': 213500298\n",
    "}\n",
    "clf = RandomForestClassifier(**hyperparameters)\n",
    "\n",
    "top_k = 1000\n",
    "\n",
    "label_col = 'quickstart_label'\n",
    "entity_col = 'entity_id'\n",
    "date_col = 'as_of_date'\n",
    "exclude_cols = [label_col, entity_col, date_col] # columns to exclude from the X matrices for the classifier\n",
    "\n",
    "protected_attribute_col = 'poverty_level'\n",
    "\n",
    "# Parameters for Aequitas\n",
    "\n",
    "metrics = ['tpr']\n",
    "disparity_threshold = 1.3\n",
    "protected_attribute_ref_group = {protected_attribute_col:'lower'}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model and predict on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "y_train = split1_traindf[label_col].values\n",
    "clf.fit(split1_traindf.drop(exclude_cols, axis = 1), y_train)\n",
    "\n",
    "# test set predictions\n",
    "split1_preds = split1_testdf[[entity_col, date_col, label_col]].copy()\n",
    "split1_preds['predict_proba'] = clf.predict_proba(split1_testdf.drop(exclude_cols, axis = 1))[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a quick look at the predictions to make sure they look good:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split1_preds.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine predictions with protected attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(split1_preds, split1_attrdf, how='left', on=[entity_col,date_col], left_index=True, right_index=False, sort=True, copy=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sort by score, then split by protected attribute (poverty level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "protected_attribute_groups = df[protected_attribute_col].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sort_values('predict_proba', ascending=False)\n",
    "subgroup_dfs = []\n",
    "for grp in protected_attribute_groups:\n",
    "    subgroup_dfs.append(df[df[protected_attribute_col]==grp].copy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate within-subgroup cumulative recall\n",
    "\n",
    "Here, we calculate the recall up to and including each row within the highest-poverty and lower-poverty subsets of the test set. Doing this allows us to recombine and sort the sets in a way that will let find recall-equalizing \"top k\" list sizes for each subgroup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for subgrp_df in subgroup_dfs:\n",
    "    subgrp_df['cumsum_recall'] = subgrp_df[label_col].cumsum() / subgrp_df[label_col].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recombine the subgroup sets and sort by this cumulative recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_df = pd.concat(subgroup_dfs, axis=0).sort_values('cumsum_recall', ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find subgroup sizes, holding the overall list size (1000) constant\n",
    "\n",
    "Now we can simply threshold this re-sorted list by `top_k` (here, 1000) to identify how many individuals from each group we should apply in the future.\n",
    "\n",
    "Notice here that each subgroup will still be ordered by their predicted score, but the scores will no longer be perfectly ordered across subgroups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_pp = recall_df.head(top_k).copy()\n",
    "new_pp.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_pp[protected_attribute_col].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's just store this to re-use below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subgroup_k = {} \n",
    "for grp in protected_attribute_groups:\n",
    "  subgroup_k[grp] = new_pp[protected_attribute_col].value_counts()[grp]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply these subgroup-specific sizes to future test set data\n",
    "\n",
    "Now we have calculated the number of projects we need to select from each poverty level, we can apply these to the most recent split to assess how well this method reduces the recall disparities we saw initially and whether this has any impact on the overall precision of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the predictions and protected attributes from the future test set\n",
    "\n",
    "Note that the predictions here correspond to the same model + hyperparameters we specified above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split2_preds = pd.read_csv(DATAPATH + 'predictions_c598fbe93f4c218ac7d325fb478598f1.csv.gz', compression='gzip')\n",
    "split2_attrdf = pd.read_csv(DATAPATH + 'test_20121201_20130201_protected.csv.gz', compression='gzip')\n",
    "\n",
    "df2 = pd.merge(split2_preds, split2_attrdf, how='left', on=[entity_col, date_col], left_index=True, right_index=False, sort=True, copy=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a quick look to make sure the data loaded without any issue:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the test set by poverty level to apply the thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df2.sort_values('predict_proba', ascending=False)\n",
    "new_subgroup_dfs = {}\n",
    "for grp in protected_attribute_groups:\n",
    "    new_subgroup_dfs[grp] = df2[df2[protected_attribute_col]==grp].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose the number of projects from each subgroup found above\n",
    "\n",
    "Notice here that we're choosing the \"top k\" individuals within each subgroup based on their predicted score -- in a deployment, we wouldn't know the true labels to calculate recall values, which is why we had to go one step back in time to find these group sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp_dfs = []\n",
    "for grp in protected_attribute_groups:\n",
    "    pp_dfs.append(new_subgroup_dfs[grp].head(subgroup_k[grp]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recombine and create a predicted class label for this resulting set\n",
    "That is, 1,000 projects with a label 1 chosen by this process and 0 otherwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_pp2 = pd.concat(pp_dfs, axis=0).sort_values('predict_proba', ascending=True)\n",
    "\n",
    "mitigated_df = df2.copy()\n",
    "mitigated_df = mitigated_df.rename(columns = {label_col:'label_value'}) # naming for Aequitas\n",
    "\n",
    "# create a \"score\" column with the predicted class (named \"score\" for use with Aequitas below)\n",
    "mitigated_df['score'] = mitigated_df.apply(lambda x: 1.0 if x.name in new_pp2.index.tolist() else 0, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for comparison, let's also look at the unmitigated result again..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unadjusted_df = df2.sort_values('predict_proba', ascending=False).copy()\n",
    "unadjusted_df = unadjusted_df.rename(columns = {label_col:'label_value'}) # naming for Aequitas\n",
    "\n",
    "# create a \"score\" column with the predicted class (named \"score\" for use with Aequitas below)\n",
    "unadjusted_df['score'] = unadjusted_df.apply(lambda x: 1.0 if x.name in unadjusted_df.head(top_k).index.tolist() else 0, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running Aequitas - Before and After"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For the original score, without post-hoc adjustment for equity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = Group()\n",
    "b = Bias()\n",
    "\n",
    "xtab_unadjusted, _ = g.get_crosstabs(unadjusted_df[['score', 'label_value', protected_attribute_col]].copy())\n",
    "bdf_unadjusted = b.get_disparity_predefined_groups(xtab_unadjusted, original_df=unadjusted_df, ref_groups_dict=protected_attribute_ref_group)\n",
    "\n",
    "ap.disparities_metrics(bdf_unadjusted, metrics, protected_attribute_col, fairness_threshold=disparity_threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For the score, with post-hoc disparity mitigation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = Group()\n",
    "b = Bias()\n",
    "\n",
    "xtab_mitigated, _ = g.get_crosstabs(mitigated_df[['score', 'label_value', protected_attribute_col]].copy())\n",
    "bdf_mitigated = b.get_disparity_predefined_groups(xtab_mitigated, original_df=mitigated_df, ref_groups_dict=protected_attribute_ref_group)\n",
    "\n",
    "ap.disparities_metrics(bdf_mitigated, metrics, protected_attribute_col, fairness_threshold=disparity_threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, it looks the post-hoc adjustments have actually manage to mitigate the existing disparity pretty well (perhaps even over-shooting somewhat, though still within our fairness threshold of 1.3).\n",
    "\n",
    "However, the natural question here is where there is a fairness-accuracy trade-off here: What cost did we incur in terms of model performance, that is overall precision?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Before: Precision of the original, unadjusted score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unadjusted_df.loc[unadjusted_df['score']==1]['label_value'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### After: Precision of the new, disparity-mitigated score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mitigated_df.loc[mitigated_df['score']==1]['label_value'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Somewhat surprisingly, we actually don't seem to see any trade-off with the disparity mitigation here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding to the model selection (tradeoff) graph\n",
    "\n",
    "Finally, let's look at how this new option stacks up against what we plotted in our model selection process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mitigated_precision = mitigated_df.loc[mitigated_df['score']==1]['label_value'].mean()\n",
    "plot_configs = {\n",
    "    'evals_df':evals_df, \n",
    "    'aequitas_df':aequitas_df,\n",
    "    'attr_col':'poverty_level', \n",
    "    'group_name':'highest',\n",
    "    'performance_col':'model_precision',\n",
    "    'bias_metric':'tpr', \n",
    "    'flip_disparity':True, \n",
    "    'mitigated_tag':'Equalizing Recall',\n",
    "    'mitigated_bdf':bdf_mitigated, \n",
    "    'mitigated_performance':mitigated_precision, \n",
    "    'ylim':None\n",
    "}\n",
    "create_scatter_disparity_performance(**plot_configs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
